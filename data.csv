,0
0,"This article was written by members of TM Forum’s Closed-loop Anomaly Detection and Resolution Automation project. Authors include: Tayeb Ben Meriem, Senior Standards Manager, Orange  Emmanuel Otchere, Chief Architect, Huawei; Amandeep Singh, Technology Architect, IBM; Sharath Prasad, Data Scientist IBM; Utpal Mangla, VP & Senior Partner, IBM.This is the third blog in the series from the TM Forum Closed-loop Anomaly Detection and Resolution Automation (CLADRA) project. The first blog covered how closed-loop management can help communications service providers (CSPs) tackle network challenges and improve customer experience. The second blog covers closed-loop Patterns with focus on OODA (Observe-Orient-Decide-Act) approach and shares logical architecture and it’s mapping to OODA layers.This third blog covers the core processes dealing with data and AI Models as part of CLADRA Logical Architecture and the way it is mapped with AI closed-loop-driven ODA (Open Digital Architecture) as prescribed in GB1022. However, the intention is to apply this mapping to Autonomous Network Architecture Framework as well, once available. In both cases, the goal is to identify communalities and gaps to prevent duplications and use those AI closed-loop capabilities as common generic enablers to be used and consumed by all those architectural frameworks.AI models and trainingAI models are the brain of the closed-loop automation process and data is the fuel of the models to make it operate within a cognitive business and cognitive service assurance fashion.There are a number of different steps involved in creating, training and operationalizing these models. These include:Data collectionData compositionTraining of modelsValidation and testing of modelsDeploying and monitoring modelsTo help CSPs streamline efforts in their AI journey, and maximize the benefits of AI closed-loop automation, we propose that the above steps must be subjected to the AI Checklists assessment exercise as described in IG1239.The data collection process consists of collecting data from resources and services. The resources consist of both physical infrastructure such as networking and storage systems as well as virtual infrastructure such as firmware, operating systems etc. Services include functional services that are executable containers or applications; software programs or an implementation of software that performs a function for the organization, user, service, or other resource. The data collected from these systems include operational data, performance data, usage data and context data.The data composition process performs the crucial role of converting the incoming data into valuable information and actionable knowledge. The data composition process is responsible for building information across data sources, and consists of cleaning, formatting and organizing data. The components in this layer allow for connection to, and managing of the data sources as well as all required data conversions.Once the data is collected, cleaned, transformed, and composed, it is ready to be used to train a model. The AI systems needs to be trained to identify and resolve any issues. Depending on the use case, the training data could be time series data, trouble tickets, run books and a variety of other data sources. A number of techniques may be applied to train a model including machine learning and deep learning algorithms. Supervised learning algorithms such as neural networks, k-nearest neighbor, and decision trees are often used. Unsupervised algorithms such as K-means, autoencoders, principal component analysis and hypothesis tests-based analysis are also employed.Once the model is trained, one needs to validate the models which involves measuring the quality of the model and fine-tuning the model hyperparameters. The validation dataset is used to evaluate and compare performances of multiple trained models and decide which works best. Once the best model is selected, it is tested on the testing dataset. There are several evaluation metrics such as accuracy, precision, recall etc.Once the model is tested, it is deployed. Deploying a machine learning model means to integrate a machine learning model into an existing production environment where it can take in an input and return an output. In the closed-loop Automation context, the purpose of deploying your model is so that you can make the predictions from a trained machine learning model available to other systems to then take informed actions on various target systems such as network devices/ operating systems/ applications etc. Systems and data change with time, so a model created yesterday may not be accurate. It is therefore essential to ensure models remain accurate, free of bias and can accommodate for changing data environments. Therefore it is important also to monitor these deployed models to ensure the models remain fair, explainable and compliant.AI Closed-loop-based ODA Intelligence Management Framework Figure 1 (below) depicts the ODA Intelligence Management Framework as described in GB 1022. In this Framework, AI, autonomics and cognitive capabilities are built-in per design in the Intelligence Management functional block, and in each of the three ODA horizontal functional blocks (Party Management, Core Commerce Management, and Production).Both types of decision making elements, or decision elements (DEs / AI models), are either logically centralized or distributed control software logics (DEs) that operate in different timescales, but interwork harmoniously in realizing these autonomic behaviors: Self-configuration, self-optimization, self-healing of managed entities.Figure 1:  ODA intelligence management (AI closed-loop-based ODA Architecture) Ref GB 1022 Mapping between CLADRA and AI closed-loops-based ODA In Table 1 below, we compare and contrast CLADRA (IG1219) with ODA Intelligence Management (GB1022)Table 1: Mapping between CLADRA and AI closed-loop-based ODA Takeaway #1We can see ODA and CLARDA Frameworks are aligned. Both are multilayer (hierarchical and nested AI closed-loop-based frameworks composed with two layers: Slow control loops and fast control loopsMapping CLADRA use cases with AI closed-loop-based ODA architectureThis mapping can be broken down into two categories:CLADRA use cases pertaining to AI closed-loop-driven business assurance. It can be hosted by ODA Party Management and ODA Core Commerce Management functional blocks)CLADRA Use cases pertaining to AI closed-loop-driven service assurance. It can be hosted by ODA Production functional blockTakeaway #2ODA Intelligence Management makes a clear separation between “cognitive business assurance”, and “cognitive flexible catalog assurance” in one hand, and “cognitive service assurance” in the other. This offers two categories of ‘platforms’ for hosting and operationalizing the two categories of CLADRA use cases 1) “AI closed-loop-driven business assurance use cases”   2) “AI closed-loop-driven service assurance use cases”Mapping of AI closed-loop-based ODA Functional Architecture with CLADRA Logical ArchitectureFigure 2 depicts such a mapping. We indicated in both architectures the “OODA” steps by their respective symbols: Observe (Ob); Orient (Or); Decide (D); Act (A); Security (S); Automation (Au) Figure 2: Mapping of AI closed-loop-based ODA Functional Architecture with CLADRA Logical Architecture   Takeaway #3It is a high level mapping in the sense that we considered only the ‘layer’ levels without considering component levels associated with each layer. We notice there is a nice mapping between CLADRA Logical Architecture and AI closed coop-based ODA Functional ArchitectureLooking forwardAI Closed-Loop Automation Reference Architecture (TR284) captures key items for future consideration to be addressed in the next iterations/sprints of the CLADRA project.  Item 1) titled “Mapping to ODA” – which constitutes the main content of this blog, along with data and AI models – aims to provide an initial high-level scoping. The next blog in this series will be dedicated to scoping the API view of AI closed-loop-driven ODA intelligence management.We invite you to join our current study on closed-loop automation, with further details to be found here."
1,"As enterprise systems and processes begin to embed 5G connectivity, the nature of service assurance will change.Future service assurance systems will work on cloud native platforms to provide the agility and scale required by operators and their customers. AI and machine learning will provide intelligent contextual analytics, and if you are to become service-centric rather than network-centric, you will require an end-to-end view of all service-specific data.Join us for the webinar and hear the experts sharing insights on:How to break down siloes and legacy processesHow to drive out complexity caused by the multiplicity of data sources from the network and support systemsAnd why the cultural change will be as challenging as technology evolution"
2,"Maintenance of critical production line equipment at Ford Motor Co. can slow down the manufacturing process, costing significant time and money. Too much maintenance time can be as bad as too little, so Ford and its equipment suppliers are seeking ways to optimize maintenance schedules through analysis of data from 5G-connected sensors attached to the machines. Combined with related technologies such as machine learning and augmented reality, 5G has the potential to transform the way manufacturing equipment is maintained.Intelligent maintenance is a key use case for the 5G Enabled Manufacturing (5GEM) project, a consortium of eight partner organizations (ATS Global, Ford, HSSMI, Lancaster University, TM Forum, TWI, Vacuum Furnace Engineering and Vodafone) exploring use cases for 5G private networks in different manufacturing environments. The project, supported by the UK government’s 5G Testbeds and Trials Program, is developing 5G-enabled intelligent maintenance for vacuum furnaces in a test environment at TWI near Cambridge, UK.Car manufacturers use vacuum furnaces to bond two metals using a filler metal that melts at a lower temperature than the metals being joined. Properly maintaining the furnace is vital to ensure a high-quality bond, but maintenance typically requires the machine to be taken out of service, interrupting production. Ford and its manufacturing partner Vacuum Furnace Engineering are using 5G connected sensors to remotely monitor the vacuum furnace’s performance, state of health and environmental factors in order to streamline the maintenance process.As Ian Jenner, Director of Control Systems at Vacuum Furnace Engineering (VFE), describes in this Q&A, predictive maintenance based on data can save a manufacturer time and money.Ian Jenner, Director of Control Systems at Vacuum Furnace EngineeringWhat problems are you trying to solve with intelligent maintenance?There are many expensive machines with complicated maintenance schedules involved in automobile manufacturing processes. We don’t want to carry out unnecessary maintenance which causes downtime on the production line, but equally we don’t want to wait until there is a fault, which can cause longer and more expensive delays.It’s much more efficient to use predictive maintenance, whereby data from sensors attached to a machine tells you when it’s about to need servicing. With this project, we’re collecting large quantities of data from vacuum furnaces in different environments and processing that data centrally at VFE using machine learning and AI to develop predictive maintenance algorithms together with Lancaster University which can be deployed in the factory.Project partner ATS Global uses the 5G network to deliver a hybrid cloud directly to the shopfloor, enabling the 5GEM consortium partners to deploy AI at the network edge. 5G-enabled sensors attached to machines on the production line then feed the predictive maintenance algorithms in real time to indicate the optimal maintenance schedule for each machine.Why are you using 5G-connected sensors?Firstly, 5G provides a highly secure connection, which is important because we are collecting commercially sensitive data from our customers’ sites and processing it centrally. In the 5GEM project we are using a 5G mobile private network (MPN) supplied by Vodafone, which gives Ford full control over who has access to their network and their data. They can work confidently with trusted suppliers and partners such as VFE and TWI.Secondly, in order to feed the predictive maintenance algorithms deployed in the factory we need to collect large quantities of data from multiple sensors in real-time. 5G provides high bandwidth connectivity to support these requirements. With an MPN we are not limited by the network design choices relevant for public mobile network users, so in principle we can tune the network to support the higher uplink bandwidth needed for our use cases.Sensor measurements from a vacuum furnaceWhat are the other applications of 5G for intelligent maintenance?We’re also using 5G-connected AR/VR headsets to provide remote expert support to our customers.  With this setup we can guide on-site staff to fix relatively simple faults which would previously have required a callout of a VFE engineer to the factory.  5G’s low latency is critical for a practical and comfortable VR experience.What challenges and opportunities lie ahead?We are still somewhat constrained by the lack of standardized, in-built connection points to install and power sensors. In the future, we envisage supplying machines with such connection points ready for 5G sensors to be added as and when needed. For example, if a machine is performing poorly, we can quickly add extra sensors to monitor its behaviour remotely in real-time to diagnose the fault.We are also looking to build digital twins of processes to help remotely located experts work out solutions without stopping the production line.Downtime can cost £100,000 [$138,000] per day in lost production for high-value vacuum furnace processes such as diffusion bonding. Ultimately, we are looking at 5G-enabled intelligent maintenance to save our customers significant money in downtime and scrapped loads.This is the second in a series of interviews about the 5GEM project. In the first, Chris White, Manager of 5GEM at Ford, explains why enabling real-time process analysis and control in manufacturing is critical."
3,"The combination of 5G and edge computing promises to create new services, new value propositions and new lines of revenue. Telecoms operators are still trying to figure out:What services they should be developingWhere they sit in the value chainWho to partner withRegardless of partnerships they forge, edge computing will generate huge volumes of data which they can then exploit to optimize their networks, deliver better customer experiences, and build new products and solutions.This webinar explores what operators need to do to get the most value out of their 5G data and how they can become valuable partners in new 5G and edge ecosystems."
4,"This article was written by members of TM Forum’s Closed-loop Anomaly Detection and Resolution Automation project. Authors include: Aaron Boasman-Patel, Vice President – AI & Customer Experience, TM Forum; Emmanuel Otchere, Chief Architect, Huawei; Mathews Thomas, Executive IT Architect, IBM; Satishkumar Sadagopan, Associate Partner / Executive Architect, IBM; Utpal Mangla, VP & Senior Partner, IBM; Vikrant Bhargava, Deputy General Manager, Bharti Airtel. Find the first in this three-part series here.Please also register for the webinar – Enabling the networks & services of tomorrow using AI closed loop automation – for further insights.Closed loop patterns evaluation Industry, from the time of steam/water powered mechanization to mass production with electricity and information communication technology (ICT) based systems, use control systems in automation. In a knowledge-based environment, service providers must bridge physical and digital operations together in order to manage services and experience at scale and speed. Control systems such as the ancient Ktesibios’s water clock in Alexandria, Egypt – since invention around the third century BCE – have been instrumental to manage, command, direct or regulate the behavior of “systems” for deterministic outcomes. These systems heavily depended on human intervention. To make control systems reliable and dependable, and thus release autonomous behavior in real-world conditions, automation requires intelligent feedback patterns to be introduced in order to effectively address changing conditions, while reducing low-level human involvement.There are several feedback patterns, which help to effectively engage and govern real world circumstances with their changing conditions. A good feedback pattern provides a framework that can effectively embed the right part of the output of a control system, at the right stage, back into the system using an intelligent decision planning process. Some patterns provide basic feedback analysis, while others have evolved based on the complex nature of decision making. The choice of a system for any use case may differ, but there is general consensus that feedback systems need to integrate some forms of intelligence to address the modern-day complexity.Choosing the Observe-Orient-Decide-Act (OODA) closed loop patternTo identify a generic pattern that can support the complexity of managing anomaly detection and resolution across disparate and integrated systems (expert systems and domains), the OODA Closed Loop pattern was selected. Others like Monitor Analyze Plan Execute (MAPE), Foundation Observation Comparison Action Learn, rEason (FOCALE), Plan-Do-Check-Act (PDCA) etc. were analyzed along with OODA by asking some relevant questions that included:The organization of the framework – Is it for purely for technical use, or operational, business, etc.?The pattern design pivot – Is it one node centered, a system of nodes focused etc.?How “dynamic” is the pattern? Is the pattern able to address static information paths? Dynamic information paths? Is it adaptive? Can it support recursive feedbacks?Is the pattern able to support self-organization?Does the pattern include context awareness of the environment?How well is the pattern documented for reference and adaptation?Based on the analysis, merits and scope of various closed loop patterns, the Closed Loop Anomaly Detection and Resolution Automation (CLADRA) project identified OODA as an exhaustive and yet generalist pattern to enable adapt to different anomaly detection and resolution use cases.  ETSI GANA, FOCALE and CASCADEs can be used too as they are extensions of OODA. However, OODA in its purest form provides a generic framework to model the common entities.By leveraging AI, the OODA Closed Loop pattern presents exhaustive feedback loops that address the different stages of decision cycle with “fast loops” and “slow loops” reflecting the different decision points that represent intelligently managing the automation of anomaly detection and resolution.CLA Architecture alignment with OODA The CLADRA reference architecture provides a validated framework that is vendor or product agnostic. It can be used to automate anomaly detection and resolution through standard functions, components and interfaces.For eTOM-based service and resource management to utilize closed loop automation, it is very important to identify and develop logical, physical and operational architectures based on practical designs, implementation and operations experience.In our effort, we leveraged the best practices contribution from CSPs, network equipment vendors, independent software vendors, and consulting & systems integrators to rationalize the components required for CLA in the service and resource domains of eTOM. It also highlights the layers, integration reference points and general specifications that address the ambitions of CSPs seeking solutions, and solution providers looking for standardized language of requirements to work together using a set of well-defined business and technical objects.As reference architectures, it is not compulsory to implement the full stack of functionalities, components or interfaces. However, it is important to consider an end-to-end approach to design and implement closed loop architecture for anomaly detection and resolution knowing the target architecture. This is the added value the report offers. By starting with simple closed loop implementations, CSPs, vendors, consulting and system integrators etc., can better frame a comprehensive, pragmatic roadmap for fully automating, and effectively managing the lifecycle of CLADRA systems.Logical architectureReferring to the figure below, four key functional layers for closed-loop anomaly detection and resolution have been identified: Data collection, data composition, decision execution and process execution. The logical architecture also supports a number of cross-cutting capabilities (including security and automation) that need to be enabled in every layer and governed consistently across all layers.This allows for a loosely coupled architecture, with the facility to implement the functional modules with the best of breed components or custom components to best suit each of the components.The data collection layer maps to the “Observe” stage of OODA, and connects and collects data from resources and services. This includes data from compute, storage, network, applications, interfaces etc.The data composition layer maps to the “Orient” stage of OODA and builds information across data sources through analysis, dissemination and reflection. This is where data is processed, cleansed, normalized and modeled.The decision execution layer maps to the “Decide” stage of OODA where the AI/machine learning (ML) models are trained and executed. Based on the outcome form the AI/ML modes, appropriate decisions are made on the best resolution/action.The process execution layer maps to the “Action” stage of OODA and models and executes the processes and actions determined by the “Decide” layer, through process orchestration, human-related workflows, robotic process automation etc.The security layer runs across all the above layers and secures the solution from various aspects of infiltration and vulnerability, such as, user security, data security, application & infrastructure securityTaking action As a use case led project, members adopt a rigorous review and validation of the architecture to enable practical industry best practices and standards. By using the OODA feedback framework, we intend to identify relevant data models, APIs and automation standards that will be used and improved by TM Forum member organizations. Feedback received from the membership will be used in subsequent iterations of this work to further develop and improve the architecture, integration into ODA and realization of standard automation objects.Some of the key items for future consideration are:Mapping to ODADevelopment of assets, like APIs and data models (as extensions to SID)We invite the reader to join our current study on Closed Loop Automation; further details can be found here."
5,"The network and service operations of the future cannot continue to operate with the current traditional approach, especially with the virtualization and dynamic nature of the network with virtual network functions (VNFs), cloud-native network functions (CNFs) and software-defined networking (SDN).The rate at which these virtual networks can scale up/down, the exponential growth of the data and events produced by the network (both physical and virtual) and the associated complexity requires a solution that can detect anomalous patterns in real time. In addition to this, they also need to continue to learn and improve over the lifecycle of the solution.Service providers need solutions that can automate the majority of their operations, reduce human error, improve operational efficiency, and help deliver innovative, uninterrupted superior quality services to their consumers.Join this Global Architecture Forum webinar to learn how TM Forum, with its members, has developed an AI Closed Loop Automation Reference Architecture as well as process flows around key use cases, including alert correlation for operations, systems performance prediction and traffic flow optimization.Aaron Boasman-Patel of TM Forum will be joined by leading key industry figures including Airtel’s Vikrant Bhargava, IBM’s Mathews Thomas and Satish Sadagopan and Huawei’s Emmanuel Otchere."
6,"This article was written by members of TM Forum’s Closed-loop Anomaly Detection and Resolution Automation project. Authors include: Aaron Boasman-Patel, Vice President – AI & Customer Experience, TM Forum; Emmanuel Otchere, Chief Architect, Huawei; Mathews Thomas, Executive IT Architect, IBM; Satishkumar Sadagopan, Associate Partner / Executive Architect, IBM; Utpal Mangla, VP & Senior Partner, IBM; Vikrant Bhargava, Deputy General Manager, Bharti Airtel.Please also register for the webinar – Enabling the networks & services of tomorrow using AI closed loop automation – for further insights.If customer experience in the field of telecoms was a term few years back, it has now become a core for driving engineering and network operations today. The challenge is not only to provide superior customer experience, but to do it using a complex interwoven microservices architecture. This requires the use of a framework as a key lever for automating engineering and network operations.Closed-loop Anomaly Detection and Resolution Automation (CLADRA) refers to a TM Forum framework that contains a reference architecture and related collateral to enable communications service providers (CSPs) to transform network operations by using AI driven closed-loop automation to detect anomalies, determine resolution and implement the required changes to the network within a continuous highly automated framework.Network challengesEngineering and network operations are extremely challenging for the following key reasons:Servicing customers is no longer just about applications or devices, but about business service which includes end-to-end observabilityAn enormous volume of data is now being generated across ecosystems in terms of logs, alerts, metrics etc.Network management is no longer about fixing the failure but about predicting the potential failure. Reactive is outdated – it’s all about predictiveThe above challenges make it extremely difficult for engineering and network operations to be able to quickly correlate and analyze multiple application performance metrics to solve complex emerging problems before they impact end-user experience. There needs to be a framework which can drive efficiency in the network, predict problems, drive automation and ultimately improve the user experience.Closed-loop automation capability frameworkClosed-loop automation helps solve any problems before they even become issues. The following are some of the key capabilities of closed-loop automation to transform engineering and network operations into a predictive and automated operations model:Anomaly detection: This enables the capability to ingest and process large data in real time. Also, anomaly detection makes use of time series data to analyze applications, networks, operating systems, database metrics etc. This gives anomaly detection the capability to identify patterns and anomalies and raise awareness towards predictive actions.Intelligent alerts: In a general operations environment, 20% of overall alert volume is false positive. These false positives add to the overall load and volume of operations teams. closed-loop automation uses machine learning models to create the patterns for the series of alerts so that those can be bound to causes and known actions and corrected accordingly.Root cause analysis: Closed-loop automation leverages data to intelligently identify all anomalies in the service path and use AI to map it to find the most likely cause for a particular incident. During the course, it makes use of various AI algorithms to ensure accuracy of root cause identifications and implements the required remediation stepsPredictive planning: With this capability, CSPs can predict how application and network behaviors are dependent on seasonality and other factors, to ensure appropriate corrective actions are taken, thereby permitting systems to perform optimally.TM Forum is currently conducting a study on CLADRA and the following are some use cases members of the study have deployed:Traffic flow optimization: Identifies network traffic issues by analyzing past and current traffic issues; uses AI to identify solutions and implements the correction using the management and orchestration layer to achieve a self-healing network and efficient network optimization.Alert correlation for operations: Identify and correlate alerts using machine learning so that they can be bound to known causes and be used for remediation of alertsSystem performance prediction: Real time capacity forecasting for hardware components on the basis of application parameters considering seasonality and network loads to ensure capacity is available.Further details of the above and other use cases can be found here.Closed loop automation overviewClosed-loop automation can address issues of varying complexities and the implementation will vary depending on the problem being addressed. The figure below provides an overview of closed-loop automation addressing issues of varying complexity:Some issues are relatively simple to resolve and the network can be modified easily to implement the desired changes. This is often done by analyzing the data and invoking the orchestration engine to make the appropriate changes to the network components.In other cases, it is important to predict issues which could happen in the future. The appropriate data is analyzed by various predictive models which then make a recommendation on the change to be made to the orchestration layer which implements the change.In complex cases it is important to combine the predictive insights information with additional AI systems to determine a resolution. The AI system has been trained to resolve these issues and is integrated with a robotics automation system to automate the process. If the AI system determines it has a high confidence that the suggested resolution is correct, it will invoke the orchestration engine to implement the solution automatically.  If not, a trouble ticket is generated, and an engineer will resolve the issue.Sample closed-loop automation managementBenefitsThe following are some of the benefits of closed-loop automationImproved network reliability through automation built on AISuperior customer experience leading to reduced customer churn and improving the bottom lineManual tasks are reduced through automation hence raising workforce productivityMean time to resolution for incidents are decreased providing improved network services, better network performance and a faster rollout of new servicesWe hope you have a better understanding of the importance of closed-loop automation, its associated use cases and the benefits of implementing it.  The next blog will take a deeper dive into the key components behind closed-loop automation and a high-level architecture.  We invite you to join our current study on closed-loop automation, with further details to be found here."
7,"When it comes to customer experience, some operators are happy with modest improvements while others are seeking to develop a digital experience that rivals that of cloud providers. Explore what a fully digital, data-driven customer experience is and what is at stake for those CSPs that are taking a less ambitious approach."
8,"This is an excerpt from out report Enterprise 5G: The role of the telco. Download it now for the full insight.Communications service providers (CSPs) may be tempted to jump headfirst into the enterprise 5G business, but they won’t all be successful. In countries that have four or five national mobile operators, it is likely that only one or two will have enough experience serving enterprises to succeed. An operator whose business is 95% consumer today will find it difficult to build B2B capabilities that require different skills, experience and market access. For operators that do decide to target enterprises with 5G services, here are some important steps to take:Consider connectivity Many CSPs believe that to grow revenue they must develop new capabilities and offer services beyond connectivity. But 5G’s ability to deliver low-latency connectivity and speeds that come close to those achieved over fiber mean there is a huge opportunity to grow basic connectivity. For small and medium-sized enterprises this could be as simple as selling 5G to provide resiliency or redundancy, or basic broadband connectivity for new or temporary sites. But operators will have to improve the connectivity experience to be successful. This means offering the ability to customize or configure connectivity to meet the requirements of specific users or applications, and providing a fully digital experience that ideally puts the customer in control of managing their own connectivity requirements.Don’t be a followerOver the past 30 years, mobile operators around the world have built very similar networks using similar strategies in terms of deployment, services, pricing and support systems. But many CSPs will go their own way in developing networks and strategies to support the B2B market. They will focus on specific markets, particular customer bases, and they will develop business models that will work for them. As such, there will be no 5G enterprise blueprint, and operators will need to choose their own paths rather than following others.Weigh partnershipsThe telecommunications industry has been talking about the value of partnerships for several years without always being clear about their commercial rationale. CSPs partner with over-the-top (OTT) providers to increase loyalty and customer lifetime value. They do not make a profit from the services that they resell, and the deals may even be loss-making. When it comes to partnerships for 5G enterprise services, operators could take the same approach. Partnering with hyperscale cloud providers may be the best way to sell more 5G connections. On the other hand, CSPs may choose partnerships that will let them claim a bigger share in the sale of full, end-to-end solutions or that allow them to own the relationship with the enterprise customer.Let customers leadCSPs have long adopted a “build it and they will come” model, but this approach will not work in the enterprise market where each customer has a specific set of requirements. Mobile operators are finding special rules, characteristics and approaches to deployment of technology in every vertical. Without engaging potential customers to work through these challenges, it will not be possible to decide which network capabilities to exploit or to develop, or which support systems will help them monetize the opportunity.Strike a balanceWhile operators will need to spend time investigating the requirements of verticals, they will not be able to build a profitable business if they have to customize every solution. CSPs should also focus on developing horizontal capabilities that are repeatable across sectors. For example, an operator could sell security-as-a-service to companies in all kinds of industries.Choose verticals wiselyEven the world’s largest B2B telecoms operators are discovering that they may be able to serve only a few verticals with end-to-end ICT solutions. To become service providers of choice in those sectors, CSPs may need to acquire specialist ICT providers that already have expertise (and customers) in a given market. Vodafone is a good example of an operator that is using this approach as it has committed to serving business customers in the sometimes overlapping automotive and insurance industries.Decide role in MPNsThe explosion of interest in mobile private networks (MPNs) has forced CSPs to quickly put together market propositions that meet the many and varied requirements of enterprises. Operators likely will focus on the same verticals in their MPN strategies as they do in selling 5G services more broadly. Forging strong partnerships with systems integrators and network equipment vendors will be essential for operators to participate in MPNs.Build for agility5G gives mobile operators the opportunity to innovate and co-create with business customers. While customer centricity is essential, operators will also need flexible network and IT architectures. Slow, costly customization and integration will not work. Enterprises necessarily will take an experimental approach to exploiting 5G’s capabilities, so mobile operators will need to adopt cloud native technology and move workloads to the public cloud in order to deliver flexible, scalable solutions. TM Forum members are developing the Open Digital Architecture (ODA) as a blueprint for modular, cloud-based, open digital platforms that can be orchestrated using AI. ODA replaces traditional operational and business support systems (OSS/BSS) with a new approach to building software for the telecoms industry. To find out how ODA can help in targeting enterprise 5G, please contact George Glass, and for more information read the white paper below.Let customers leadCSPs have long adopted a “build it and they will come” model, but this approach will not work in the enterprise market where each customer has a specific set of requirements. Mobile operators are finding special rules, characteristics and approaches to deployment of technology in every vertical. Without engaging potential customers to work through these challenges, it will not be possible to decide which network capabilities to exploit or to develop, or which support systems will help them monetize the opportunity.Strike a balanceWhile operators will need to spend time investigating the requirements of verticals, they will not be able to build a profitable business if they have to customize every solution. CSPs should also focus on developing horizontal capabilities that are repeatable across sectors. For example, an operator could sell security-as-a-service to companies in all kinds of industries.Choose verticals wiselyEven the world’s largest B2B telecoms operators are discovering that they may be able to serve only a few verticals with end-to-end ICT solutions. To become service providers of choice in those sectors, CSPs may need to acquire specialist ICT providers that already have expertise (and customers) in a given market. Vodafone is a good example of an operator that is using this approach as it has committed to serving business customers in the sometimes overlapping automotive and insurance industries.Decide role in MPNsThe explosion of interest in mobile private networks (MPNs) has forced CSPs to quickly put together market propositions that meet the many and varied requirements of enterprises. Operators likely will focus on the same verticals in their MPN strategies as they do in selling 5G services more broadly. Forging strong partnerships with systems integrators and network equipment vendors will be essential for operators to participate in MPNs.Build for agility5G gives mobile operators the opportunity to innovate and co-create with business customers. While customer centricity is essential, operators will also need flexible network and IT architectures. Slow, costly customization and integration will not work. Enterprises necessarily will take an experimental approach to exploiting 5G’s capabilities, so mobile operators will need to adopt cloud native technology and move workloads to the public cloud in order to deliver flexible, scalable solutions. TM Forum members are developing the Open Digital Architecture (ODA) as a blueprint for modular, cloudbased, open digital platforms that can be orchestrated using AI. ODA replaces traditional operational and business support systems (OSS/BSS) with a new approach to building software for the telecoms industry. To find out how ODA can help in targeting enterprise 5G, please contact George Glass, and for more information read this white paper.Improve BSSToday’s enterprise BSS stacks are simply not designed to exploit the versatility of 5G, and it will be a while before operators understand fully which 5G capabilities are most relevant to the verticals they are targeting. It is clear, however, that 5G services will require CSPs to participate in digital ecosystems. In addition, enterprises expect control, self-service and real-time capabilities. CSPs need to start deploying these capabilities today to ensure they are ready when enterprises move beyond experimenting with 5G. Again, the ODA can help operators reimagine their core commerce systems for agility. In addition, TM Forum members are working on a Business Capability Map and Business Architecture to help CSPs understand their unique capabilities and how to monetize 5G services. To learn more about this work, please contact Joann O’Brien."
9,"Sponsored by: ComvivaIndosat is one of the largest telecommunications providers in Indonesia, where the majority of consumer customers are prepaid, churn is high, and customer loyalty is low due to a lack of service transparency. Subscribers typically have limited access to comprehensive pricing details, costs can change based on region, and unexpected charges or fees can occur based on data consumption, low balance, or lack of usage.ResultsThrough the use of customer data analysis and AI/machine learning-based modeling, Indosat was able to better predict customer behaviors for recharge/top-up, data usage, inactivity and next best offer responses. Along with the launch of a multi-tier loyalty program, Indosat saw substantial churn reduction of its high value customers using the loyalty program, and a 10X increase in revenue linked to real-time marketing campaigns.Download the full case study below to learn in more detail how Indosat, with the help of Comviva, achieved such substantial results, and watch here as Mark Newman, Chief Analyst TM Forum talks to Ritesh Kumar Singh, (CMO) and Carlos Eduardo Quiroga Tarradelles, (CVM Head) at Indosat Ooredoo.Read the full case study"
10,"This article was contributed to Inform by a member of TM Forum .Build your cloud native data lake 30-40% fasterAll DSPs require a high-quality data storage and analytics solution which offers more agility than conventional systems. A serverless data lake is a popular way of storing and analyzing data in a single repository. It features autonomous maintenance and architectural flexibility for diverse kinds of data, which in turn accelerates integration with analytics engine and improves time to insights.Inefficient data lakesDSP data lakes today do not seem to meet expectations due to reasons such as:The data lake becoming a “data swamp”Lack of business impactIncreasing complexities in data pipeline replicationParameters to build an efficient AWS serverless data lakeListed below are a few critical parameters which can help DSPs mitigate these challenges and implement a scalable and flexible data lake enabled by serverless technologies.1. Data architecture workshop – A consultant facilitated a ‘business value first’ approach at the workshop where the DSP is guided towards architected principles including:Identifying the highest priority business cases with strategic cost/benefit analysis and incorporating the elements into architecture and design.Building flexibility into the design to address the changing needs of the business, IT, and other functions as requirements and use cases evolve.Constructing a long-term architecture blueprint with serverless technology elements, integrations, performance and scalability.Identifying and prioritizing the use cases that provide business units with maximum value from data lake is a critical success factor.2. Interface control template – Interface control templates assist firms in integrating the numerous interoperating services that come together in a modern data platform. The templates capture the interactions between the services and how they work in tandem with each other. Having an interface control template is critical to establish an event-driven orchestration in the data lake as it provides a standard way of integration across the services. The pre-defined integration procedures further help in reducing any rework needed for data lake implementation.3. Infrastructure as code (IaC) – The effort of building a pipeline can be reduced using event-driven, infrastructure-as-code applications for diverse source data systems – NOC, CMS, enterprise data. This is because IaC makes use of reusable templates. IaC is a paradigm for provisioning and managing serverless applications through a series of software using a cloud configuration orchestrator. Using the orchestrator helps to spawn new pipelines with the necessary Amazon Web Services (AWS) resources. Instead of recreating the entire pipeline for minor changes, resources can be effectively spawned by changing a few configuration files in the orchestrator.Fig 1: Infrastructure as Code (IaC) to execute data pipeline engineering sprintsInfrastructure as Code (IaC) techniques can make pipeline build & replication process 60% faster.4. Data cataloging approach – DSPs today are moving towards more generalized data lakes where raw data is gathered from various sources and stored in its native format. Due to a lack of governance, descriptive metadata, and a mechanism to maintain it, the data lake becomes a convoluted data swamp. To avoid data swamps, DSPs could follow the steps below:Metadata: Create metadata using a ‘glue crawler’ which helps to populate the AWS Glue Data Catalog and crawl multiple data stores in a single run.Data catalog: Create a data catalog of where the data lies and the path traveled by the data. This is used for lineage tracking.Single hierarchy structure: Due to multiple handoffs and stakeholders, hierarchies in the data lake end up inconsistent and confused. Hence, ensure that a single hierarchy and set naming conventions are used across the data sources to reduce complexity. As Amazon S3 is a global namespace, we recommend firms explicitly choose specific names or locations for the datasets to be stored and transformed.Data governance: Since data from different sources are brought to a centralized data lake, we recommend that any identity and access management procedures hold stringent access rules.5. Event-driven orchestration – DSPs usually have multiple data pipelines and data sources for different use cases. Managing the complexity of multiple pipelines is key to managing data lakes. Event-driven orchestration using AWS step function enables serverless queries and serverless polling, as it has features to poll for the ‘extract, transform, load’ jobs and triggers any necessary next steps upon completion. The step functions orchestrate the sequence in which the job needs to be run, and assist in automating the end-to-end workflow of the data pipeline. Event-driven orchestration in a serverless data lake ensures end-to-end automation of data flows and data transformations, which improves the total data processing time by 40%.Fig 2. Event-driven orchestration to automate data transfer and transformationConclusionServerless technologies built natively on cloud, benefit from cloud-scale innovation and provide flexible architectures for a variety of use cases beyond just data lakes. By implementing the best practices elaborated in this insight, a leading DSP in Latin America was able to realize benefits such as:Accelerated time to build a fully operational cloud native data lake by 30-40%Quicker launch of analytics and machine leaning use cases, with a 50-60% reduction in machine learning integration effortsSignificantly less costs than infrastructure as a service or on-premise data lake infrastructure"
11,"Sponsored by: SwimNetwork data is key to capturing the customer experienceView the related webinar on-demand here.Dozens of communication service providers (CSPs) operating around the world have tens of millions if not hundreds of millions of subscribers. Operating at this scale has its own challenges when trying to understand the unique customer experience of each subscriber. On any peak day, over a 100 million subscribers equates to more than one billion voice calls and over 10 billion text messages being transmitted across the telecommunications networks – and these complex networks require a significant amount of infrastructure and support to operate.Deployed on cell towers or other locations across the entire service region, the network infrastructure itself generates a vast amount of data that encapsulates the customer experience on a regular, ongoing basis. Think of the radio access network (RAN) equipment for 4G and 5G automated equipment (eNodeB’s for 4G LTE and gNodeB’s for 5G NR) and how it’s operated and maintained by professional teams made up of thousands of field technicians and engineers. They depend on the data generated by this equipment to manage and support those network operations.Yet, the benefits of understanding the network – both for interpolating the customer experience and for operating the network in support of those customers – can be elusive. The sheer volume of data generated by network equipment equates to millions of messages per second with data transfer rates in the order of tens of Gigabits per second. Storing the raw data would quickly amount to several petabytes of data every day.The one constant we can be assured of going forward is that the volume and complexity of this data will only continue to grow, especially with the continued deployment of 5G technology and the increasing use of edge applications.Understanding network data is critical for the businessAcross the organization, every business operation within a CSP could make better use of network data to more positively impact the customer experience. From helping subscribers join the network with new devices, measuring and relaying their call metrics, and providing timely customer service and support – every aspect of network data can be used to influence their unique experience. And when it comes to managing the entire lifecycle of the infrastructure – including planning, installation, programming, and maintenance of equipment – all the way to the operators, field technicians, and support crews – this data can be used to help provide quality network operations.And finally, teams examining market share across consumers and businesses can quickly and dynamically understand trends in the network data to help describe customer experience and set market expectations.What’s really needed is continuous intelligenceWith today’s modern business needs, CSPs should be designing their data architectures for continuous intelligence, allowing them to:Continuously ‘listen to’ changes in network data and, in some cases, even project outcomes into the near futureProcess and analyze both historical data and inbound messages simultaneously so that insights are delivered in contextEnable their IT systems to always have an answer and accelerate or automate decision making in the momentWhile historical analysis and data exploration may be useful for periodic reporting or root cause analysis, it doesn’t enable the kind of ‘always-on’ situational awareness that businesses need to handle these massive data volumes from network infrastructure.Build a real-time mirror of your networkContinuous intelligence platforms, like Swim Continuum, are designed with this goal in mind. These platforms give CSPs the ability to build a real-time mirror of their network entirely from the ground up. They start by integrating and aggregating data from static sources, like databases and data lakes, and streaming technologies, like Apache Kafka and Pulsar, regardless of their location in the cloud, edge, or in-between. Every aspect of the telecommunications network can then be created and linked together. And as configurations dynamically change within the network in real-time, these platforms immediately reflect the current state of the network and the CSP’s business.These platforms also allow CSPs to perform contextual analytics that apply their business logic and key performance indicators (KPI’s) while processing data in real time. By continuously inferring and learning across data models, users can quickly discover trends and project outcomes in the near future. And finally, these platforms allow CSPs to visualize their data in real time through dashboards or other applications, allowing them to be alerted and respond in real time to customer needs.Case study: Results from a global telecommunications providerThe organizational benefits and positive impact of achieving continuous intelligence can be seen today through the experience of a global telecommunications provider. They leveraged the Swim Continuum platform to provide:Accessibility – Unified data access across their organization, allowing them to onboard different business users and teams simultaneouslyPerformance at scale – Processing 100’s of KPIs on over 100 million different network elementsSpeed – Delivering insights on streaming data in context with millisecond latencyVisualization – Providing a ‘single-pane-of-glass’ view of insights and system performance accessible anywhereInfrastructure – Flexible deployment across resources in the cloud and at the edgeAs a result, the CSP achieved their goal of having all business KPIs available and updated continuously, in real-time.Real-time insights to enhance customer experienceGoing forward, choosing continuous intelligence for your data architecture will quickly become the standard for CSPs looking to optimize and enhance the customer experience for long-term loyalty and success. And its impact will stretch across every aspect of your business operations, such as geospatial analytics to identify service outages from significant weather events or to plan network upgrades for better coverage. Continuous intelligence can also be used to identify and highlight anomalies on the network, helping ensure network quality in terms of data speed, call quality, and call reliability. And finally, continuous intelligence maps the entire customer experience – from calculating real-time performance scores that might influence phone or service upgrades, to pre-emptive notifications for subscribers moving into low coverage areas.Want to know more, access the related webinar on-demand now."
12,"No one expected 2020 to turn out the way it did, that’s for sure. By February, Covid-19 had turned the world upside down. Telcos rushed to move their call center employees to work remotely. City skyscrapers became vertical ghost towns and their dedicated T1s seemed pointless. The world’s businesses were running on home Internet services. Webcams became HTF (hard to find). “You’re on mute!” was the quote of the year. And suddenly we could all relate to the BBC reporter who was interrupted by his dancing toddler and baby on wheels during a live broadcast a few years ago.With all of that in mind – and fully aware that annual predictions from companies are a bit of a cliché – I thought it would be interesting to find out what TM Forum’s subject matter experts think we can expect from 2021, and how the pandemic might influence their predictions. Read what they had to say, then let us know your predictions. Tweet #2021predictions @tmforumorg.George Glass, CTOThe ecosystem of the telecommunications industry is about to undergo it largest transformation in decades. The time is right for cloud native networks to emerge from the laboratories. We have been exploiting, and enjoying, the benefits of configurable storage and compute on demand, i.e. cloud native IT, in the systems space for several years now, and it is time for networks to catch up! The worlds of systems and networks are merging, and everything is becoming software applications that run on standardized, virtualized infrastructure.We have seen the start of network infrastructure virtualized, and we have seen the emergence of software-defined network components. But, when these two capabilities come together and are exploited from a software engineering perspective, we will start to see truly cloud native network functions that sub-divide today’s network components into configurable and orchestratable network services. Running on public cloud infrastructure will become the way to dynamically deliver, deploy and manage the networks of the future.These virtualized, software-based network functions will need to be deployed using concepts, such as dynamic workload allocation, so that the relevant network function can be moved, while in-service, to the appropriate cloud-based domain (hyperscale core cloud, edge cloud or device cloud) depending on the requirements of the service, such as low latency or elastic scalable capacity.John Gillam, Chief Digital OfficerNormally, I’m not a fan of making predictions, but after living through 2020 and the challenges of leading global teams during Covid-19 and ubiquitous remote working, I couldn’t resist taking part in this year’s fortune telling exercise. Based on my experiences this year, I think 2021 will see advances in real-time digital human interaction spurred on by necessity as well as a backlog of serendipitous moments waiting to happen at water coolers around the world – once the world gets back to the office.Let’s face it: Right now, Zoom and Teams are a stopgap solution for collaboration, rapid innovation and Agile software development – five years from now we will probably look back and consider them primitive, but they did let us get the work done.Still, we are all missing those “water-cooler moments” where a new idea emerges from a casual conversation – an idea that may change the world. After almost a year of remote working, people are anxious to connect at a deeper level than on Teams and Zoom. They are anxious to get back to work, face-to-face. And after having a thorough understanding of what’s lacking in digital human interaction, our industry’s more diverse brainpower and increasing use of crowdsourcing and open code is truly an ideal environment for rapid innovation and iteration. Hope to see you around the virtual water cooler at Action Week starting February 1.Aaron Boasman-Patel, Vice President, AI and Customer ExperienceData regulation has again been thrown into the spotlight, especially due to the pandemic and the inability to access meaningful data which can be shared. That’s why I think 2021 is going to see a renewed focus on data regulation around the globe. The ability to share large data sets is also key for the adoption of smart technologies which will see massive growth in 2021. TM Forum continues to look at this space carefully and will be releasing a new data governance whitepaper in March.Another one of my predictions is focused on AI regulation and how it’s used and implemented across systems, networks and applications. When Ursula Von der Leyen first took the European Commission presidency in December 2019, she promised a legislative proposal on AI within the first 100 days. While this hasn’t been met, once the pandemic is under control, it will be a renewed focused for the world. However, managing and governing AI, which includes trying to eliminate bias, is a huge and difficult task, not only for operators and enterprises but for governments as a whole.Addressing model provenance and audit, bias, correctness and comprehension will also come to the fore as will model ethics and delegation. Here at the Forum, we are continuing to work on AI governance, recently published AI Model Data Sheets and checklists, and are currently working on an AI Canvas, or framework, to ensure that AI can safely be deployed and managed across the lifecycle.Joann O’Brien, Vice President, Digital EcosystemsThe global pandemic has been a driver for digitization across many industries, especially healthcare, education, government and retail, and it will influence global cross-industry relationships for years to come. Operators can no longer look at the world like the man with a hammer. They need to be asking, “What can I do for you?” now, or they will finally lose the war with over-the-top providers.Under the “what can I do for you?” category, I think connectivity-as-a-service (CaaS) will be a 2021 buzzword. We are definitely entering a new era of sustainable development, renewable energy, sustainable farming and conservation around biodiversity, oceans, food systems and climate. Operators need to be relevant to these challenges and provide seamless CaaS, including scalable IoT infrastructure, and partner to deliver services and build trust with these industries.As the pandemic continues, digital health will also boom in 2021. Driven by focus and direction from the World Health Organization, operators will concentrate on growing partnerships, relying more on 5G, and developing methodologies and common capabilities that accelerate innovation in digital healthcare systems, and in turn, business assurance, fraud prevention, cyber security and more.Vicky Sleight, Director, Human Factor and Diversity & Inclusion Council2020 was a year that none of us could ever have expected. Organizations learned how to be flexible and harness the diverse voices in our industry – not just to make them heard, but to be involved. They also learned how include diverse thoughts, perspectives and experiences to drive innovation across products and services in a way they have not done before.With employees isolated, businesses had to quickly learn that everyone’s personal circumstances are different and take a much more integrated approach to wellbeing and inclusion. In fact, the CIPD revealed that organizations choosing to proactively support the health and wellbeing of their workforce can lead to much more positive results. Going forward, executives see empathy playing a far greater role in sustaining a thriving workforce; and not surprisingly, two out of three employees would prefer to work for an organization that shows empathy toward all of its stakeholders.As business leaders prepare their workforce for the next phase of recovery, they must ask the question: how do we cultivate a healthy, productive and empathetic workforce willing to own the transformation journey – and resilient and sustainable enough to handle the challenges brought by a global pandemic? For sure, innovative and agile approaches to work are key components of survival through crisis. Collaboration, trust and mutual support are also critical to team and organizational endurance, along with continued flexible working.We will reach our common global goals only if we are able to create equal opportunities for all, address the failures exposed and exploited by Covid-19, and apply human rights standards to tackle entrenched, systematic, and intergenerational inequalities, exclusion and discrimination. TM Forum will continue to support the industry to achieve this through our work within Human Factor and the Diversity & Inclusion Council as we work toward our goal of making the tech and telecoms industry the most diverse industry in the world."
13,"Sponsored by: CSGAs technology continues its evolution and changes the way communications service providers (CSPs) operate, it is imperative to stay ahead of the curve. This is particularly true where digital mediation – the process of capturing, correlating and converting raw usage data into actionable insights – is concerned.Digital mediation is an essential component for future cloud infrastructure, and CSPs need to start preparing for the future today. This eBook covers the three main mediation challenges CSPs must overcome to stay ahead of the curve and the competition—and the three ways to conquer these challenges.Download the eBook"
14,"As 5G connectivity becomes embedded in enterprise systems and processes, many of which are business-critical, the very nature of service assurance will change. Future service assurance systems will work on cloud native platforms to provide the agility and scale required by operators and their customers. They will embed AI and machine learning to provide intelligent contextual analytics. And if they are to become service-centric rather than network-centric they will require an end-to-end view of all service specific data.Join this webinar to explore how telecoms operators are going about reimagining their approach to service assurance in the 5G era.Operators will need to drive out complexity caused by the multiplicity of data sources coming both from the network and support systems. Hear the experts sharing insights on breaking down siloes and legacy processes, and the required cultural change."
15,"Find out more about this project or take part by contacting [email protected]. The Covid-19 outbreak has stress tested cities’ and governments emergency response abilities. In some regions where a data driven approach has been used, authorities have been able to gain the agility needed to get ahead of the virus. Given the very nature of virus and emergency situations a more automated and data driven approach is essential across multitude of services to help manage the ongoing situation.  There is an inexplicable link between climate and health emergency as put forward by Dr James Hospedales of Earthmedic. Destruction of plantation exposing new viruses to humans not previously encountered, to air pollution impacting quality of life and health.As the late, Denis Gilhooly of Global [email protected] 2030 put forward, there is a basic need for a globally coordinated, collective and multi-stakeholder response to the “next pandemic”.  The Covid-19 pandemic lay bare the shortcomings in our ability as a global community to come together in tackling what is the greatest public health challenge of our time. Digital Health is critical to pandemic detection, prevention, response, and recovery through its unique and inherent ability to keep pace with the velocity, ubiquity and real time nature of the pandemic. We must learn from the experiences of Covid-19. We need to leverage every available resource, strive for data driven and automated responses to enable societies to get “ahead of” the virus or emergency and allow continuity of services.We saw that certain regions handled the pandemic much more effectively than others. A TM Forum Catalyst proof-of-concept project seeks to explore and evaluate how a common framework for trusted data sharing could be created and adapted for emergency management, while operating inside the regional regulatory variations. A key consideration is the citizen and the need for consent driven citizen data solutions, with a common core framework and natural extensions for regional variants.The project aims to build upon a previous iteration which took the following approach in a single geographical region: Where Covid-19 was identified and reported by urban hospitals, the platform allowed the Centers for Disease Control (CDC) to use telecom data to analyze where patients have been and who they have been in close proximity to so that at-risk contacts can be traced and urged to isolate to curb the spread of the virus.Data was aggregated from the CDC, police, transportation companies and telco operators to analyze and manage crowd flows and logistics to minimize virus transmission. Heat maps based on telco data can provide real-time monitoring to understand how people are moving around and to prevent crowds gathering.The big data platform also enabled targeted support for vulnerable people such as children, the elderly and people with disabilities.This Catalyst demonstrated how the global pandemic could speed up digital transformation for cities through trusted data-sharing.The team will show how the platform can be useful for managing not only the immediate emergency and its lasting impacts, but also for addressing future challenges and opportunities.  The next phase of work is to explore how to do this on a global scale operating inside regional regulatory contexts.If you’re interested in learning more about the project and/or taking part, please contact TM Forum’s Vice President of Digital Ecosystems directly at [email protected]."
16,"Read our related report AIOps: From automation to autonomous networks for the full  insight.Because AIOps for CSPs encompasses AI, operations automation and autonomous networks, it is easy to conflate AI and automation, failing to recognize one is a technology domain and the other an approach to business processes and IT. AI is increasingly playing a role in the automation toolkit, either as a way to analyze and optimize processes.Though AI and automation are related, “they are still not intertwined,” says Mohammed Fahim Momen, general manager, OSS & customer insight, for Robi Axiata.  “Rules and input-output based software engines are powerful enough to execute many basic as well as advanced tasks which are not necessarily AI by definition,” Momen says.Momen agrees that the hype around AI tends to equate it with automation but thinks each “will continue to generate value based on different use cases.”Automation without AICSP processes tend to be siloed even though there are often dependencies across silos. There may be some interconnectivity to address these dependencies, but the silos and processes continue to work independently. For the specific job functions within those silos “traditional software does give us significant leverage” Momen says. Such operations systems’ functions can express decades of domain expertise effectively. They typically use no AI at all to automate portions of operations process, like design, planning, provisioning or service deactivation.There are also proven use cases for traditional operations software that are AI-like but involve no AI.  “Self-organizing networks, anomaly detection, process control and governance, insight and reporting and many more use cases based on traditional software are yielding excellent results without any AI,” Momen explains.When AI winsThere are areas, however, where a consensus of confidence has emerged around AI’s superiority over traditional software. Momen and others interviewed for this study say AI is the best option for processing extremely large data sets that combine heterogeneous data from multiple sources and where multi-domain or cross-functional correlation is required.For example, “360-degree assurance of network performance and customer experience requires a unified platform and solution and there the need for AI arises,” Momen says. AI may also be a better fit than traditional software for automating responses to continuously changing network configuration and customer experience-related requirements.Where to focus automation effortAutonomous networking use cases are driving initial interest in AIOps, inclusive of those instances where AI is not actually part of the automation architecture. CSPs know it will typically take a few steps along the automation path to increase the degree of zero-touch service they can offer and ultimately to arrive at autonomous networks.Given the complex and often disparate state of many CSP’s operations environments, there are basic criteria for automation CSPs can use to determine where to start. Most of the hard work is practical and detail oriented. “It’s not like you plant a seed and grow an autonomous network,” says María Eugenia Armijo Marchant, Platform Implementation Expert, Telecom Argentina.Armijo Marchant advises those approaching operations automation and AI to take several key incremental steps, including:Understanding and identifying processes & operations that are well suited for automation.Improving these first with “simple automation.”Gathering data to understand the automated operations environment better.Testing AI-assisted automated interventions and measuring whether there are gains and improvements.She warns peers not to take automation’s complexity lightly and urges those who seek to automate operations processes to respect the “mandatory conditions” for effective automation. “The more deterministic a process is, the better automation will fit it,” Armijo Marchant says.For a closer look at how CSPs are using AIOps to automate processes and AI to being the move toward autonomous networks, download the latest report from TM Forum: AIOps: From automation to autonomous networks."
17,"TM Forum surveyed a panel of operations experts from more than 20 CSPs, consultancies and supplier organizations around the world to gain a picture of AIOps relative to adoption, maturity, direction and real-world implementation.74% of the respondents to our survey work in operations for CSPs in hands-on managerial roles in IT and operations. 64% of respondents work for CSPs with more than 5 million subscribers, while 41% work for CSPs with more than 25 million subscribers.Despite AIOps early phase, nearly half (46%) of our survey respondents report that they have at least an evolving AI business case, while another 40% have none at this stage.AIOps remains a very new term for most CSPs, as only 30% of respondents report hearing the term used in their companies at least weekly, while more than half (54%) say they never hear it.Compared with data collected in TM Forum’s 2018 AI survey, CSPs are slightly more focused today – 53% – on using AI to automate a single process within a single domain than two years ago –44%. This may reflect the practical challenges faced when automating processes across organizational silos and boundaries.Yet AIOps is firmly in the early part of the adoption curve among CSP operations teams.  54% of our survey respondents are using AI today to automate a single operations process within a single domain. More than 33% of respondents are using AI to automate one or more processes across multiple domains, which aligns with our findings from 2018 – 36%.AI market still taking shapeThere is not yet a widely adopted management approach to AI. 0% of our survey respondents report a C-level owner and only 14% report having a VP-level owner for AI. 33% report multiple owners, because AI is cross-functional. Another 32% say it is unclear who owns what, especially with AI.Similarly, IT spending related to AIOps may yet be modest. Figure 12 shows 47% of our survey respondents say AIOps will occupy less than 10% of their IT spend in operations during the next 3 years. For 35% of respondents, however, AIOps will command one fourth of their budget. A small portion of respondents – 12% – say because AI will be part of everything, all spend will go to AIOps.CSP AI activity present but modestMore than half of our survey respondents say they have no AI activity in network planning and management today, though nearly 25% are building AI into products and services in this category. Almost half have no AI activity in service creation and management, though roughly 35% have completed proofs of concept in this area.By 2022, our survey respondents expect to see more forward momentum in AI deployments. Nearly 35% of respondents see AI being used in network planning and management. Service creation and management may see only small gains in adoption, however, with less than 20% of respondents saying it will be implemented by 2022.AI tools still maturingCSPs still face maturity hurdles with AI as a consumable enterprise software product. Not only are AI tools immature, but the environment around them may not have transformed enough yet to feed them. For example, network automation is a killer app for AI. But 69% of our survey respondents say the biggest barrier to network automation is a lack of standards for exposing network operations data.While there’s great excitement around what AI can do, there will be a learning period for CSPs to be able to unleash its full potential. AIOps is gaining ground because it provides a basis for finding ways to improve operations using both automation and AI. 60% of our survey respondents say automation is a primary objective for their network and service operations.For a closer look at how CSPs are using AIOps to automate processes and AI to being the move toward autonomous networks, download the latest report from TM Forum: AIOps: From automation to autonomous networks."
18,"Transformation impacts every aspect of a communications service provider’s (CSP’s) business, including operations which have undergone constant change since the mid­1990s. In that time, their role has altered from offline records related to telephony services to processes that design, install, provision, activate, maintain and manage inventory for every network­-based service. AI in operations, or AIOps, envisions a high level of AI­-assisted or AI-­driven automation in IT and network operations.For this report we surveyed a small, targeted panel of operations experts who are active in TM Forum’s collaborative work on AIOps. We wanted to gain a picture of their adoption, direction and real-world implementation of AIOps, and a measure of its maturity. A large majority of the group we surveyed work in operations for CSPs in hands -on managerial roles.Read the report to understand:Why a lot of automation does not need AIWhere AI can be most usefully introduced and howWhy new technologies and autonomous networks are major drivers of AIOpsThe benefits of automated networks enabled by AIOpsThe challenges of implementing AIOpsLearnings and success stories from CSPs around the world, including China Mobile, Orange, PCCW/Hong Kong Telekom and Telecom ArgentinaThe role of TM Forum’s Open Digital Architecture and Open APIs"
19,"The Digital Transformation World Series (DTWS) may be over, but one of the overarching themes will be relevant for years to come – that is, how industry collaboration can transform a service provider business to meet the demands of a cloud-native world.Hot topics included 5G monetization, managing AI at scale, the importance of Open APIs in building an Open Digital Architecture, and how diversity and inclusion can transform R&D. But, what stood out the most, was that over 8,000 attendees from 157 countries and 700 unique companies came together to see innovative solutions, start valuable conversations and partnerships, and inspire change.The event’s Catalyst proof-of-concept project demos and packed masterclasses exemplified an industry’s willingness to collaborate, upskill and reskill.As Nathan Bell, Chief Digital Officer of M1 Limited said after the event, “[DTWS was an] awesome demonstration of how people can come together across the globe and share their experiences despite the majority of us working from home.”Over the next month, we will push some of the event’s best digital content live on our YouTube channel. Here’s what you should start with so you’re well-prepared for the challenges of 2021 and beyondMust see CxOsThe six-week agenda featured a star-studded line-up of telecom CxOs. Topics ranged from how to cope with connectivity in times of crisis, turn ambition into reality and pivot business for success.Stéphane Richard, Chairman and CEO, Orange, spoke about the software revolution.Mukesh Ambani, Chairman and Managing Director of Reliance Industries Ltd., shared insight into Reliance Jio’s 5G rollout.Claudia Nemat, Board Member, Technology and Innovation, Deutsche Telekom AG, explored how to create a culture of resilience.Ruza Sabanovic, Executive Vice President and Chief Technology Officer of Telenor, covered the art of the telco pivot in a digital ecosystem.Enrique Blanco, Chief Technology and Information Officer, Telefónica, shared the three key pillars of his 5G strategy.Bharti Airtel’s Group CIO Harmeen Mehta shared how Airtel is supporting countries throughout the Covid-19 pandemic.Entrepreneur, author and C3.ai CEO Tom Siebel, who is credited with inventing the roughly $40 billion worldwide CRM market, shared that he sees big opportunities for CSPs in AI.Masterclasses for the massesFor the first time, TM Forum offered 12 masterclasses to all attendees. Led by TM Forum and industry experts such as Stuart Birrell, Former CIO of Heathrow Airport, Lester Thomas, Chief Systems Architect at Vodafone Group and Rachel Higham, Managing Director of IT & MD of Asia for BT Technology, the classes focused on helping collaboration among hundreds of people to solve business challenges facing the industry today.Classes looked at:Diversity and inclusion: How inclusive design drives innovationLaying down winning foundations with AI, analytics and automationHow CSPs can help verticals go digitalLeveraging ODA and Open APIs to achieve digital transformationBuilding customer confidence and trustLearn from the CxOsThe closed-door CxO Summit brought together 100+ industry executives who are leading their transformation journeys. Get a sneak peek by watching the highlight videos:CxO Summit 1: Failing fast, learning fasterCxO Summit 2: Building a 2025 technology organizationCxO Summit 3: Beyond connectivity – Growth at the edge Festival of Collaboration resultsThe Collaboration teams made progress on projects that align with the events’ six themes, and focus on key challenges such as:AI, Data & Analytics: AI Checklist Cards for guidance on AI procurement, development, deployment and end-of-lifeAutonomous Networks & the Edge: The Autonomous Networks: Empowering Digital Transformation For Smart Societies and Industries whitepaper details progress made on creating an industry standard framework for autonomous networksBeyond Connectivity: A new connectivity-as-a-service project builds essential suites and playbooks focused on quickly and simply scaling connectivityCloud Native IT & Agility: Commencement of a new cloud native reference implementation and test platform for Open Digital Architecture (ODA) components under a new Component Accelerator projectDigital Experience & Trust: A new Customer Experience Maturity Model helps organizations map their customer experience, identify areas of improvement and put customers at the center of their business activitiesThe Human Factor: The new Diversity & Inclusion Maturity Model was established to enable organizations to have a baseline to monitor growth and effectiveness of their programs and initiativesLearn more about the event from the highlights video below: Celebrating member excellenceIt would not be a Digital Transformation World event without the annual Excellence Awards. Twelve companies celebrated their innovative achievements in implementing the Forum’s best practices and standards.Judging chair Camille Mendler, Chief Analyst of Omdia, noted that the 2020 Excellence Awards winners demonstrated some of the most unique digital transformation projects in the industry today. See the full list of winners.On the last day of Digital Transformation World Series, TM Forum, recognized the dedication and hard work of its members by naming seven Catalyst proof-of-concept projects as Outstanding Catalysts for their significant contributions to the acceleration of digital transformation throughout the industry.What’s news?Over the six weeks, the TM Forum made many exciting announcements including:Twelve new companies committed to collaboration on building the TM Forum’s ODA by signing the Open API and Open Digital Architecture Manifesto. 42 companies are have signed in total.With support from Orange and Vodafone, TM Forum and MEF announced a partnership to bring consistency and ease-of-use to standardized Open APIs, improving efficiencies and removing barriers to new business growth.You’re welcome to relive some of the great moments throughout the six weeks on our YouTube channel."
20,"Sponsored by: VoltDBMonetizing 5G is one of the hottest topics in telecoms right now, but precise answers about exactly how that will happen are scarce on the ground. Dheeraj Remella, Chief Product Officer at VoltDB, talked to Annie Turner/TM Forum about where the value in low latency lies, with answers that are precise right down to the millisecond.Access VoltDB’s latest eBook called SQL VS. NOSQL VS. NEWSQL – FOR TELCO.Dheeraj Remella, Chief Product Officer, VoltDBFirst, can you tell us more about what VoltDB does and how?VoltDB is an in-memory data technology platform that facilitates data-driven AI. So maybe Cloudera or Snowflake or whatever engine runs the machine learning and generates a model that you import into VoltDB, then run it as part of your transactional processing. VoltDB is about bringing all those elements together, to execute accurate, automated, actionable decisions in under 10 milliseconds (ms).A good example is a Chinese multinational vendor that supplies fraud management software. By running our technology on its real-time data platform, this company enabled a customer to apply the 1,500 rules that are necessary to determine if a credit-card transaction is fraudulent or not. As a result, that customer is able to reduce fraud by 83% while increasing the capacity to handle transactions by a factor of 10.This is because we can handle millions of transactions per second and our decision response time is in single-digit milliseconds. Users run VoltDB to run their rules codified within VoltDB and can operate at high scale and super low latency.The usual alternative is to kick off a project with a bunch of developers to build code, then implement it and return the machine-learned results. Typically, by the time you’re done interpreting what you have discovered from the process and operationalizing it, that learning is already in the archival store.How does the technology play into monetizing 5G?5G is going to almost mandate bringing that level of intelligence to a family of use cases closer to the edge. The two kinds of edges that have the highest likelihood of becoming key to monetizing 5G are on-prem[ises], and network edge.The network edge is provided by the mobile network operator, and we can think of examples like AT&T and Azure working together in the US to bring Azure edge zones to market. Likewise you have Verizon working with AWS Wavelength which is essentially a colocation data center with Verizon’s network data center and they call this the network edge.The idea is you’ll have six data centers in a city, say, and you probably have a 10ms round trip to them. Right now, operators worldwide are working to bring that time down from 20 or 30ms to single digits. We’re asking what is the point of a low latency ping round trip if the value creation in the middle of that trip takes an exorbitant amount of time? If value creation is slow, there’s no value in it.We have low latency communications, we need to explore how that affects operators’ digital transformation and their customers’ transformation? How does it help industrial automation and the enabling and usefulness of digital twins, for instance? How do we join all these dots together to provide value towards automation?What are we talking about in terms of time regarding value creation?A single millisecond round trip with 500ms to create value is too slow. The value creation is in the automated decision-making, which, depending on the situation, can involve personalization, business process, decisioning, real-time control feedback, and maybe telemetry data.If you’re using telemetry data to run a wind farm and something goes wrong, you need to act fast to power a windmill down before the torque does any damage. That’s what you want rather than collecting all the data then trying to decipher what happened. In real life, the windmill carries on, the torque builds up and could shear the aluminum tubing that houses that windmill. With fast decisioning, you can prevent it.Timing can be critical regarding customer experience: We work with a customer value management (CVM) vendor on what started as a technology optimization journey, but we discovered the hidden value of sub-10ms decisions. It turns out that when a customer contacts a mobile network operator, there is a budget of 250ms for the round trip, but a window of only 4 to 7ms to present the best-option offer to them. That offer is based on call detail records and other customer data. The rest of the time is taken up assembling the artefacts to render what the end user needs to see on their screen for them to tap the button and accept the offer. It is literally the blink of an eye in which the customer looks at their screen and makes their decision.By getting the best-option offer to customers in no more than 7ms boosted the CVM’s acceptance rates for offers by 253%. The big lesson here was that the nature of the offer and the propensity to purchase need to coincide exactly for the subscriber to take it up. That’s an example of the hidden value of enabling accurate, automated decisions in under 10ms.What do you think the main areas are for this super-fast decisioning combined with low latency communications? Enterprises need a system of multiple entities working together as a business process subsystem: They need to take streams of data from a number of relevant components and make immediate sense out of it. So far, this has proven very hard to do. We’ve been talking about extracting massive benefits from big data analytics for years, but the German word ‘Verschlimmbesserung’ sums up progress for many. It translates as making something worse by trying to make it better.Applications that leverage VoltDB could be in supply chain management or authority shipping management, container organization, or securing asset tracking. Business operations and automation, and securing your assets are not two different things; they need to be integral to each other.Security has to be intertwined throughout your entire business operations. It cannot be a separate thing that’s running on the side. That means you need the business logic to run your business optimally, and that business logic includes focusing on one thing, while preventing someone breaking into the system elsewhere. One measure to address this is only allowing agreed permissible communication protocols, not anything else.You can incorporate all this into your business logic for day to day operations or millisecond to millisecond operations, it just requires the platform that can bring both flavors of intelligence to work together. And if you if you miss either of them, if you miss the window of opportunity around an event happening, you’re behind on at least one of them.Can you talk a bit more about the potential regarding security?We’ve worked with an ISP in Japan which used VoltDB to prevent distributed denial of service (DDoS) attacks on their customers’ sites. It has a 100% record so far in stopping them before customers have been affected.In the US, there is huge fraud from ad bots which steal from content publishers. The publishers bid for real estate slots on websites, depending on who’s looking at any given moment and the information provided about that person. Ad bots sell content publishers spots on fake websites and present them with traits of non-existent people. One of our customers has been able to tackle this problem which has proved difficult previously because it all happens in milliseconds.In my opinion, we should not approach this from the point of view of a handful of use cases, but as a capability that can be applied to low latency communications, then pick a sector, any sector, and apply your imagination.Download VoltDB’s recent eBook called SQL VS. NOSQL VS. NEWSQL – FOR TELCO."
21,"Nothing may be more emblematic of the network and communications industry’s critical role in keeping businesses running and blunting the still severe economic damage wrought by Covid-19 than the fact that one of its own industry conferences took place virtually. The Digital Transformation World Series (DTWS), an annual gathering of network and communications service providers (CSPs), technology suppliers, consultancies and systems integrators hosted by TM Forum, became a virtual digital series.When the series wrapped up on November 12 after six weeks, it had attracted more than 12,000 participants – more than four times the normal number – from 173 countries. However, this was not the only such shift; the pandemic has been a positive driver for change throughout the telecoms industry, unlocking innovation and agility that we expect will continue long after the virus has gone. We also observed two trends that at first blush might seem contradictory: a new business focus on technology, and a realization that people and culture have never been more important.Three steps to unlock growthThe second half of the series focused on many of the practical requirements to unlock growth as the industry continues its transformation, including:The automation of business operations driven by data, leveraging machine learning and AIThe software revolution in network and IT that requires a single architecture and an autonomous and open approach with Open Digital Architecture (a blueprint for modular, cloud-based, open digital platforms created by TM Forum members)More evidence that people and ways of working are crucial to the transformation – while also acknowledging that the industry has a long way to go when it comes to diversity and inclusion‘Enterprise 5G’ is frequently tossed around as a catchphrase to describe the source of future growth in the industry, but the reality is more nuanced. Growth for telecom companies and network providers will likely come from three major sources and demand focused and thoughtful execution.The evolution of connectivity: Providers will need to offer connectivity that is simple, elastic and customer centric. They can do so by offering it as a cloud service, much the same way the hyperscalers offer storage and computing today. Think network splicing, not slicing: bringing together a set of telecom capabilities to underpin the right outcomes for customers, regardless of size. Edge computing almost certainly will be delivered through partnership with hyperscale cloud providers. Forum participants expect this to rely on revenue-sharing models, though there are still many details—and questions about disintermediation—that need to be resolved.Being a platform for innovation: Telecom companies must find ways to provide the core telecom capabilities that third parties, developers, adjacent markets and even internal or acquired organizations need to innovate. Links with large, global developer audiences, such as those of hyperscale cloud providers, will be crucial, which reopens the debate over federation between service providers. Speaking at the event, Harmeen Mehta, GCIO at Bharti Airtel, also focused on adjacencies and untapped opportunities, such as providing ‘human ATM’ services as a banking provider during the pandemic, or providing services to the billions of lower-income people in countries such as India.Becoming an ecosystem solution provider: Perhaps nothing demonstrates more clearly that the future of the industry goes far beyond delivering 5G connectivity than the need to partner with others to deliver end-to-end solutions to large enterprise customers. Providers will have to add multiple new capabilities to their existing offerings—and will often need to do so through partnerships.A software-first industryAnother key takeaway was the conviction that a software-first telecom industry needs to be ‘plug and play’, not ‘plug and pay’. Legacy technology, processes and ways of working are some of the greatest threats to the future vision of the industry. In addition to seeking ways to reduce integration costs and technical debt, major operators are now looking at operating costs and seeking up to tenfold improvements in operating efficiency.Participants also agreed that, like IT, AI is a strategic asset. Use cases for AI are getting more ambitious, with a focus on automation of core operations and ensuring the right outcomes for customers, rather than automating the customer interface. But CSPs are still unlikely to seize the full potential of AI on their own—or soon. Steve Jarrett, SVP for data and AI at Orange, noted that he believes it will take a decade to reach maturity. Tom Siebel, CEO and chairman of C3.ai (and founder of Siebel Systems), noted that AI should be at the top of every CEO’s agenda in what he called this “era of corporate mass extinction.” “Some will get it, some will not,” he observed.Alex Choi, SVP for research and technology at Deutsche Telekom, noted that the software revolution required companies to “break down the silos and the boundaries … and adopt a more agile software approach.” In practical terms, this means companies will have to adopt a modular, AI-ready, reusable, “open by design” standardized software architecture that leverages Open APIs, such as TM Forum’s Open Digital Architecture, as well as open source efforts such as Open RAN.A troubling lack of diversityBeyond the technology discussions, the Forum also focused on people—and on the industry’s troubling lack of diversity. The World Economic Forum found only 12% of the workforce in cloud computing businesses is female, and only 26% in data and AI, and participants at the Forum speculated that the pandemic may have made the situation worse.Even as the pandemic reinvigorated the industry with a newfound sense of purpose, a lack of diversity poses a serious challenge. It makes the industry less attractive to many employees, making it harder for CSPs to recruit the types of top talent and diverse teams that are proven to be substantially more effective at delivering on innovation. Likewise, product sets and solutions—particularly in B2C markets—need inclusive design to be successful. A lack of diversity can even cause biases to be embedded in artificial intelligence, with far-reaching consequences.Inclusion also matters. A forum masterclass on the topic noted that many diversity initiatives stop short of making sure that the diverse talent that has been recruited is genuinely engaged and included in ways that help unlock the full value of that talent. Cecilie Heuch, chief people officer at Telenor, and Peter Leukert, CIO for Deutsche Telekom, both offered their thoughts on practical and systemic approaches to cultural change. Among participants, there was a consensus that these efforts can succeed only if the CEO is truly passionate enough about the topic to make it happen and that companies must find intuitive and easy-to-measure indicators to track the success of their diversity and inclusion efforts."
22,"Sponsored by:It is not unreasonable to think of fulfillment as a standalone process. Fulfillment is the specific action executed to take a product or service from inventory and put it into the hands of the customer. It is fulfilling the customer’s need and completing the transaction. What could be more straightforward?This view of fulfillment would be reasonable, but it would be wrong.Unlike the cheese from “The Farmer in the Dell,” fulfillment does not stand alone. It doesn’t just fulfill customer needs; it completes the intent of all the processes leading up to the customer making the decision to buy (outreach, customer intelligence, targeted marketing, online portals and product catalogs) and it sets up all the processes that come after to assure customer satisfaction and quality. It also updates all the upstream and downstream processes such as inventory, billing and customer profiles. In other words, fulfillment is the main cog in the wheels of both the digital buying experience and the brick-and-mortar retail experience.It is also key to the customer experience—for better or worse. Except for instances where customers feel they were given bad or incomplete information (deception) about a product or service during the sales process – which really gets their goat – they tend otherwise to be most dissatisfied with slow service (making them wait to be served) and not getting products delivered on time as promised. So, the difference between satisfaction and dissatisfaction often hinges on the fulfillment process. Fulfillment accuracy, in the eyes of the customer, is also responsible for maintaining an optimal time to revenue.The challenge to successful fulfillment is that the networks and services it supports are becoming increasingly intelligent, and the systems it interacts with up and down stream are increasingly intelligence-based, meaning they are relying on greater volumes of more disparate data at higher and higher speeds with which to make decisions and take actions.Increasingly fulfillment and assurance are being intertwined and becoming part of the larger service and network orchestration process, or dynamic orchestration – ensuring the agreed intent of the customer service is maintained over its lifecycle. As the network evolves with 5G and becomes cloud native or at least interconnects with cloud-native networks, and services become more ecosystem-centric, fulfillment must suddenly be imbued with the ability to orchestrate across different domains and communicate with multi-party systems.This means fulfillment and the overarching orchestration process will become increasingly dependent on modernized or re-imagined operations support systems (OSS) supported by open APIs. Jason Rutherford, Senior Vice President and General Manager, Oracle Communications and Applications, said recently that by adopting the TM Forum Open Digital Architecture and Open APIs across the Oracle Communications applications portfolio, the company can provide operators with the business and IT agility necessary to compete in fast-changing markets“By leveraging Open APIs as the glue to integrate with a variety of ICT solutions from industrial partners worldwide, Chunghwa Telecom is able to accelerate the development of an innovative and thriving 5G industry chain and ecosystem,” added Heychyi Young, Vice President, Chunghwa Telecom Labs.Together, a modernized OSS and Open APIs must be able to support traditional as well as next-generation networks and solutions. They also must enable service orchestration, including the fulfillment of traditional services and next-generation cloud-based services, and support the bundling of services from partners while orchestrating the end-to-end customer request across their own and partner systems. That is a tall order for an industry that has yet to flawlessly execute such action within its own domain.With end-to-end services that might span multiple domains, one of those domains will be the customer premises, a common configuration in early multi-access edge computing (MEC) deployments. But to build and orchestrate services at the edge, operators need to understand the network edge needs of enterprises in different vertical industries.“5G opens up a lot of possibilities,” said Leonard Sheahan, Senior Director of Product Marketing at Oracle Communications Applications, “Opportunities previously not possible or non-viable may now become eminently feasible. For service providers to offer compelling 5G / MEC based communications solutions to such industry verticals, they need a deep industry expertise and relationships in order to develop differentiated propositions with industry specific appeal, a subject explored in ‘The Aviator’, a recent TMF catalyst.”The graphic below shows what enterprise verticals various CSPs hope to target.CSPs are unlikely to be successful if they simply offer generic enterprise services to these customers. Each vertical may have unique requirements and some enterprises within each vertical may have very specific needs.  As can be seen above, some service providers may elect to address a given industry with a fully verticalized solution by partnering with, or acquiring, industry players to complete the solution proposition. However, to be relevant for all industries, service providers need to tailor their network propositions to the industry vertical needs even as those requirements are only still emerging in some cases. This requires a dynamic approach to fulfillment and orchestration in general – one that can accommodate a wide variety of network capabilities based on geographic coverage, bandwidth, quality of service, etc.With this in place, the service provider may then elect to expose the appropriate degree of configurability to their enterprise customers in each vertical industry confident they can quickly deploy the desired network capability.An open architecture with open APIs is essential to orchestrating industry vertical applications and underlying network capability in ways that result in a compelling, differentiated and quality customer experience. Moreover, service providers need to deliver such applications and services from higher up in the value chain to avoid being boxed out of the lion’s share of revenue from industry verticals. Take for example how revenue sharing might play out in the case of edge computing services:As one can see, the money is not in the network. It is in the apps and services.To realize this, service providers need a modernized OSS/BSS, an open digital architecture, Open APIs, a dynamic orchestration platform, and a new ecosystem mindset to deliver on all the potential unleashed by 5G."
23,"This article covers the session entitled ‘AIOps: Redesigning operations processes to exploit the full potential of AI’ on November 11 which was part of TM Forum’s Digital Transformation World Series (DTWS). This session falls under the Forum’s AI, Data & Analytics theme.AIOps has heated up this year, especially since Gartner released its AIOps platforms market guide in December 2019. For communication service providers (CSPs), AIOps’ scope covers IT, networks and operations, and AI plays by different rules than traditional software. As AI components move into CSPs’ operational environments, we need to define how the technology should be applied, governed, managed and road mapped. The TM Forum AIOps Service Management collaboration team is doing just that.Leaders from the CSP and supplier communities sat down with Aaron Boasman-Patel, Vice President AI and Customer Experience, TM Forum, to discuss why a new AIOps service management framework is needed now.Why do we need an AIOps framework?“AI software is different from traditional software,” explained Luca Franco Varvello, ICT Senior Consultant for Huawei. “There are a lot of gaps in how to manage traditional software and how to manage AI software, so the new AIOps service management framework shall address those gaps and complement existing operations,” he said.Zhang Ke, Senior Project Manager for China Telecom added that he sees AIOps being the long-term evolution path for operations functions like automated service assurance, meaning it will be a hot topic for the foreseeable future, not a passing phase.Derek Chen, Assistant Vice President of Customer Service for Hong Kong Telecom (HKT), urged CSPs to consider not only the impact of AI in operations infrastructure, but also as part of the product set. “It’s AI for AI,” Chen said, “if you don’t have AI operations that support AI products you are crippled.”AIOps a big shift for CSPs Automated CSP operations are typically deterministic, predictable, and repeatable. AI, however, introduces non-deterministic software logic and intent-based systems to power end-to-end and even closed-loop automation. As result, “we have to move away from a traditional way of operating toward AI automation,” said Tayeb Ben Meriem, Coordinator of OSS Standardization at Orange.“That means we have to break silos that exist today – for instance from fulfillment and assurance – and we need to integrate all of this into a framework,” he said, which is the AIOps service management working group’s aim,” he added.AIOps for real time CX“AIOps is part of the whole AI strategy in Hong Kong Telecom,” said Chen. He pointed out that in the past year HKT’s over-the-top (OTT) apps business grew 26% while its premium customer base grew 8%. Hence a reactive customer support process for OTT applications was not sustainable, so HKT applied AI to predict, prevent and analyze events to fix problems before customers noticed. “AIOps happens before the customer complains,” said Chen.He added that to get started with AIOps CSPs need to look at three key areas:Standardize data so it can be analyzed: “We had cloud service, web, mobile, social media, network equipment data…all in different formats,” explained Chen.Create a dynamic customer profile. “Customers change patterns from time to time and we need to understand when they have changed so we can match on it,” he continued.Work toward real-time customer engagement. For every customer that calls in with a problem, Chen added, there are multiple other customers with the same problem who do not call at all. AIOps provides insights into how customers are using services so CSPs can engage [with] them positively, “not just because they’re calling us,” he said.“We have hundreds of millions of users at China Telecom so it’s important for us to real-time monitor the customer experience,” stated Zhang. He said the AIOps service management framework provides “a full checklist not only in redesigning traditional operations processes but also in a way that the AI-enabled software systems are monitored, controlled, and governed.”This session falls under the Forum’s AI, Data & Analytics theme which was sponsored by Cloudera"
24,"AI combined with advanced analytics, big data and virtualized computing power will drive the automation and enhancement of telcos’ network, IT, and business operations. AI models and components will be deployed into all the layers of telcos’ architecture.In this webinar, we will review case studies in autonomous networks and AI deployments, and present a roadmap for achieving autonomous networking through AIOps.Join this webinar to learn:The benefits of AI-driven autonomous networksHow the human factor plays into autonomous networks and decision makingHow collaboration teams are putting structure around injecting AI into operations"
25,"The ‘Laying the AI foundation’ session on October 21 at the Digital Transformation World Series featured a panel made up of industry leaders from BT, IBM and Vodafone, and is available to watch on-demand now. As part of the Forum’s AI, Data & Analytics theme, moderator Aaron Boasman-Patel, Vice President of AI and Customer Experience, TM Forum, led a fast-paced discussion on how operators can manage AI across the software lifecycle from procurement to decommission.Keeping AI from running amok has been a favorite sci-fi movie narrative for decades, but today it’s no joke for telecoms operators racing to automate their networks and keep up with the faster, better, cheaper demands of B2B and B2C services.Indeed, operators have a huge appetite for AI and want to know how to exploit it. But they lack the best practices and standards to avoid common traps and pitfalls, according to Rob Claxton, Chief Researcher, BT, who spoke on the panel during the event.“It’s all new,” he said. “People are having to adjust to a world that’s slightly different from the world that they’ve become used to… There are lots of traps that need to be avoided, and the processes and best practices don’t yet exist.”Checking it outBT’s Claxton has been working closely with Boasman-Patel and TM Forum members A1 Telekom Austria Group, Amdocs, Axiata, Comarch, Ericsson, Ernst & Young, IBM, Microsoft, Netcracker, Orange, PCCW, Prodapt, STC, Tecnotree, Vodafone and Zenith to develop the best practices and processes operators require to be able to handle issues related to AI regulatory requirements, robustness, explainability and risks.“What we wanted was literally something that practitioners could put in front of them and use day-to-day – a bit like the checklists you’d find in the cockpit of an aircraft,” said Claxton. “They help people across different parts of the lifecycle be prompted and reminded about [important tasks].”The result is a new low-tech tool called AI Checklist Cards that offer guidance from procurement to development, deployment, and all the way through to end-of-life. These downloadable cards are available to TM Forum members and non-members, and are intentionally simple and easy to use. A new white paper about AIOps is also free to download.If cards aren’t your game, there is also a new AI Checklist Poster that summarizes the steps. The poster and the cards work hand-in-hand with TM Forum’s AI Readiness Check, an online tool that allows AI practitioners to identify gaps between current and target capabilities across six dimensions of a communications service provider’s (CSP’s) business.Driving AI qualityTM Forum’s online AI Readiness Check – announced last year at Digital Transformation World in Nice, France – and its accompanying AI Readiness Check Poster are the first steps in starting adoption, followed by the AI Checklist Cards and poster and then AI Model Data Sheets coming early next year, according to Boasman-Patel. “Together they help drive the quality of AI management and governance practices,” he said.“There are a lot of things we can learn from best practices,” said Michael Hind, Distinguished Research Staff Member, IBM Research AI Department, IBM, during the AI governance panel. “For the data scientists and modelers and people who deploy AI, they can focus on their expertise and use the checklist to [monitor] the governance.”Another panelist, Soyini Taylor, Enterprise Data Architect, Vodafone Group, agreed with Hind on the helpfulness of the checklists.“It’s important for us to be able to note down what we have done for explainability and fairness so that if I were to give my AI model to another team, or to an outside organization, they can open the box and see what’s in it without guessing.”Claxton echoed this comment, noting that being able to document processes and steps taken in a standardized way is important for AI, especially for auditing and learning purposes. “This way we are learning about making the use of AI more effective and safer at the same time,” he said.You can watch the full session on-demand now. Watch the rest of Digital Transformation World Series content live and on-demand now too! Not registered for DTWS yet? There’s still time. Join 12,000 of your peers online through November 12. CSPs receive complimentary passes. Sign up here."
26,"We take it as a given that artificial intelligence will change the face of the telecoms industry. But how proactive do telecoms operators need to be in building the right internal skills, capabilities, and balance, between insourcing and outsourcing, to extract maximum value from AI? Where should operators direct their laser focus to extract maximum AI value and who should lead it?Is it possible and desirable to deploy company-wide strategies to leverage AI?Do telecoms operators currently have the skills to leverage AI, and what workforce innovations could aid skills gaps?When it comes to “operations” is job losses not inevitable as a result of AI?How is standardization impacted by AI?Are we doing enough as an industry to leverage AI?Sign up to access the debate.Sign up to join the debate"
27,"The performance and quality of the network impacts all aspects of a telco business. But communications service providers (CSPs) store too much of their data from the network infrastructure before they process and analyze it. This approach causes delays and prevents telecoms operators from effectively sharing customer experience insights and network data across network operations, field service and customer support organizations. As the volume and velocity of data increases, it will become even harder to derive meaningful network data insights in the timeframe businesses need and customers expect.An alternative approach is to adopt an “analyze-then-store” architecture that continuously analyzes and extracts insights from data (before storing it).During this webinar, we will explore the solutions that can help telecoms as they seek to better manage and leverage the data generated by their networks to deliver a better customer experience."
28,"Service providers are seeking ways to accelerate the deployment and activation of their fiber networks while, at the same time, eradicating errors and lost revenues. In this webinar, you will hear how one telecoms operator created a digital model of its fiber network updated with real-time data which it shared with its engineering and construction partners.Join us for this webinar to learn:How business processes need to change to break data silos between external and internal resourcesHow to create a framework for faster activation and high data accuracy of outside plant assetsHow to integrate physical network data with other operational systemsHow to use planning and construction data to manage the physical network"
29,"Sponsored by: Artificial intelligence (AI) is popular for two things: being useful and quite expensive to adopt. The future of the telco industry depends on artificial intelligence and CSPs need more innovative, secure and customized solutions. So, how can communication service providers (CSPs) monetize AI and gain market leverage? AI: your most (in)expensive employee Introducing artificial intelligence into any organization always entails costs. Often, the costs are quite high in comparison to enlisting a traditional workforce. There is a big “but” in this, though. The greatest challenge of building and maintaining network and service management systems is gathering knowledge from experts. Each professional has a different approach to addressing problems. Collecting and unifying data can be very difficult and time-consuming. Bringing in new employees puts even greater strain on the process. Adopting artificial intelligence provides specialists with a helping hand, a practical tool that takes on tedious, repetitive tasks, allowing employees to handle the crucial ones. Experienced employees are still needed – they verify the results of AI’s work, but the knowledge systemization process is all automatic. With their help, artificial intelligence can perform tedious tasks more effectively and faster than any human, which brings cost optimization.  Introducing AI: first steps Introducing AI should begin with the area where applying AI-driven automation is crucial. Since telcos should organize their data before bringing in tools containing AI, it is a good idea to have a schedule for introducing the new solutions.  Apart from that, such changes always entail some bigger or smaller issues from the technical aspect and in the human context. It’s important to prepare everything and everyone before the process begins.  If you’d like to learn more about the first steps in AI introduction, download on this white paper on introducing pervasive AI. Pervasive AI in use AI is making its way in the telco industry. Most of the telecoms are utilizing it one way or another. At Comarch, we have already had a few successful deployments of AI-driven services within our customers’ environments. The ones described below are based on real scenarios that we have experienced when working with telecom operators. Automated baseline generation and anomaly detection (ABGAD)Anomaly detection based on machine learning can identify performance indicators that do not match the expected pattern in a dataset and improve detection coverage by discovering new patterns consisting of multiple baseline violations. Traditional technologies made employees analyze multiple sources and monitor hundreds of thousands of parameters. ABGAD allows anomalies to be prioritized and cases with highest priority can be analyzed first, speeding up the entire process significantly.  This kind of automation increases the number of variables and makes them more flexible and dynamic. Apart from that, being able to link different variables and determine the symptoms enables the identification and neutralization of any problems before they influence customer experience. Automated situation detection (ASD)Automated situation detection is another great example of AI-driven technology that enables the identification of events not matching expectations and uncovering interactions between them. When a situation is detected, it can be prioritized. Cases with highest priority can be analyzed first, making the system operate at the highest possible capacity.   In this way, telecommunications companies can increase productivity and asset utilization by ensuring that situations detected in the system are identified and classified. The main advantage of ASD is the reduction of the number of events leading to the definition of a single failure. The work required by the teams will be classified according to the root cause of the situations, and the symptoms or noise that are usually reported will disappear once the source component is fixed.  Automated problem detection (APD) Automated problem detection performs root-cause analysis on trouble tickets that have been clustered into problems, which helps the user identify the root causes. The system enables problem identification that is more precise than the one performed by the experts. Machine learning provides a solution for finding potential events in the network, where any unsolved problem could lead to emergency situations. This reduces tasks for teams and causes no disruptions to the end users. It also provides the ability to find links and connect customer trouble tickets with network trouble tickets. These use cases are just some possibilities when it comes to the solutions that artificial intelligence can provide for the telco industry, especially the area of assurance and analytics. If you’d like to learn more about the AI-driven solutions for telecoms, visit our website or schedule a free demo of Comarch Intelligent Assurance – AI-powered Service Assurance Solution for Telecommunications. Comarch is a silver sponsor of the Digital Transformation World Series. While we can’t meet physically, we’ve reimagined everything you know and love about Digital Transformation World and turned it into a six-week digital event. The Series will offer a varied and engaging content program that will enable our global membership to connect, learn and collaborate, supporting all time zones. Don’t miss out on the festival of collaboration!Register today. Note: Communication service providers: you can claim your free pass here."
30,"Network as a service (NaaS) is the key to re-establishing operators as the owners of the relationship with B2B customers. To succeed in this, they need closed loop automation and AI enabled operations (AIOps) to automatically meet and maintain customers’ service needs dynamically. This means automation must span the entire hybrid network, encompassing physical and virtual elements. It also means integrating the operational and business support systems (OSS/BSS) – a goal long strived for by operators.In this Catalyst project, Boosting AIOps for full hybrid NaaS, the champions are American Tower Corporation (ATC) and Telefónica, supported by participants Everis, Verbio, and Vlocity, a Salesforce company. In this instance, ATC is the network provider and Telefónica is the retail service provider to enterprise customers.Dahyr José Vergara Suárez, Solutions Manager for Telecoms at Everis and a Catalyst team Co-leader, explains, “It is difficult to integrate the network and business areas but in our Catalyst we are merging those processes and automating them.”Importantly, the team wanted to ensure what it developed could be transposed to other services, whether wavelength as demonstrated in the Catalyst, mobile, FTTx, etc. In preparation, the team analyzed many case studies and became familiar with TM Forum’s TM Forum Business Process Framework (eTOM)  (also known as eTOM) processes. It also used the Open Digital Architecture to help with the BSS and OSS integration.Service performanceThe central issue is that while there are many models to predict events such as an overflow of traffic, and although equipment vendors can balance traffic at the device level, operators themselves have never been able to do the same at the service level in a smart, cost-effective way. This can have a profound effect on customer experience.Vergara Suárez stresses, “Data is the key component and data management must be inside the architecture for operations to be data driven and implement AIOps. You cannot do anything without data, a single data model, and an architecture that enables you to integrate all the information from different sources.”Previously, and separate from the Catalyst, Telefónica and Everis developed Fast OSS. It ingests, normalizes and models data from multiple sources, such as the underlying cloud architecture, the servers, memory and network services, the virtual network functions and software-defined network controllers.In the Catalyst, the Fast OSS is complemented by UNICA Next which analyzes the data and acts on it. For example, if the Fast OSS needs to scale up a function in the evolved pack core when a serious overflow of traffic is predicted, capacity can be duplicated or halved or whatever is necessary to maintain the right level of service – dynamically and without human intervention.This can be applied to any network function using the UNICA Next architecture, taking it up to the service level – and even to the customer level ­– because everything below in the network is automated. The provider could even smartly restart a site as an operator currently does from a network operation center.A single data modelHaving all the data within one data model means that if a server fails, it is easier to determine what the impact is and address it dynamically and automatically. The Fast OSS and UNICA Next working together can create tickets automatically, correlate information from different tickets with different network events, create new events from previous events, or create an event from network performance metrics.TM Forum’s Open APIs are the key to this integration. Vergara Suárez says, “We found that using the APIs is really good experience. Normally we have a to modify them a lot because of aspects specific to customers, but in the Catalyst, we did the opposite: We found that by doing little tweaks to the platforms and the systems, we can comply with the standard [API], and be a lot more interoperable, easily”. Everis intends to recommend this approach to customers in future.Vergara Suárez notes, “This Catalyst meets the champions’ needs and every participant is key to making that happen.”Blockchain This unusual approach to integration helped to design the resource pool to work with blockchain from scratch, using the APIs as the foundation. The blockchain establishes communications between the provider and the retailer. Vergara Suárez explains, “Normally there is a team set up on both sides of the transaction, because you need an account manager and a team as one person cannot manage everything related to billing and service level agreements.“We automated the whole contract between the two without human intervention. Whatever is recorded in the blockchain stays in the blockchain; it is communicated to different solutions and cannot be modified.”Some team members want to take various strands of the work in this Catalyst further. It is not yet clear will mean another phase of this Catalyst or continuing parts of the work in other Catalysts.Click here to watch the video of Boosting AIOps for full hybrid NaaS demonstration, as part of the Catalyst Digital Showcase, and watch a general overview of the project in the video below."
31,"This is an extract from our recent report Data orchestration: The key to becoming a data native organization. Download the report for the full insight.Many research firms have attempted to quantify the business impact of improving customer experience, but in most cases the numbers are based on theoretical models. Finding real examples of companies that been successful in translating improvements in Net Promoter Score (NPS) into increased revenue is difficult, although not impossible.At the end of last year Forrester produced a piece of research entitled How customer experience drives business growth. It assessed 14 industries, each with its own model based on the “business impact of each customer’s (dis)loyalty.” In the case of automotive manufacturers, for example, Forrester estimated that improving customer experience by one point could translate into more than $1 billion in additional revenue.Research from Gartner has found that companies soon will be valued by their information portfolios. One of its studies showed how companies demonstrating “information savvy” behavior like hiring a chief data officer, forming data science teams and setting up data governance can command market-to-book ratios well above the market average.“Anyone properly valuing a business in today’s increasingly digital world must make note of its data and analytics capabilities, including the volume, variety and quality of its information assets,” Douglas Laney, VP and Distinguished Analyst, Gartner, said in a statement about the research.The reality for most communications service providers (CSPs), however, is that their boards – while fully embracing the drive for better customer experience – are unable to trace a direct impact on overall profitability. And when it comes to their use of data, most operators are only paying lip-service to leveraging data without really understanding what is needed or indeed possible.We interviewed the CTO of a European telco who has held leadership roles in several operating companies about commitment at the board level to leveraging data as an asset “Most executives talk about how important data is, but when it comes to real commitment they are often lacking,” he says. “It took me five years to get the ball rolling in my current role – and that is only for the use of data in the network.”Real evidenceVodafone UK is an exception and the company’s success is precisely the type of case study that the telecoms industry needs to drive greater commitment to and investment in data-driven customer experience. Vodafone UK recorded quarter-on-quarter increases in revenue after leveraging TM Forum Open APIs to implement a new microservices-based digital experience layer. In 2018, Vodafone UK managed to improve its consumer Net Promotor Score (NPS) by 13 points from +9 to +22, which at the time was an all-time high. It has since risen to +33.Vodafone UK’s NPS improvement resulted from implementation of the digital experience layer, which helps the operator’s IT group roll out more than 40 on-demand production releases a day. The team has achieved zero downtime while deploying microservices into production, and total cost of ownership has been reduced as a result of reuse, adoption of cloud applications and an automated continuous integration and delivery pipeline.Optimization of non-production environments has delivered an additional cost savings of £500,000 ($621,000) per year, and the Vodafone UK digital experience layer has become the one Vodafone now uses across its entire group of operating companies. Vodafone UK also has seen measurable improvement in terms of subscriber growth and increased revenue. In the second half of 2019, its customer base started growing again after stagnating during 2017 and 2018 (as of December 2019, the company had 19.4 million subscribers). Revenues have also started to grow again, but it is more difficult to see a clear trend in churn rates (see graphic below). This would be the biggest proof point for justifying greater investment in improving customer experience.Reducing churnReduction in churn is another way of demonstrating the benefits of improving customer experience. Hong Kong-based PCCW has seen a reduction in churn from 1.3% per month to 1.1% in the last few years, although its commitment to data goes back five or six years.Group CTO Paul Berriman says PCCW’s investment in data has always been driven by a desire to improve customer experience. One of the company’s initiatives has been to partner with an analytics vendor to create a “Customer Happiness Index”, which measures customer satisfaction by analyzing several types of data including customer, network, social media and call-center data. NPS only measures customer data."
32,"Sponsored by: Blue PrismWith the novel coronavirus sweeping the globe, nearly every communication service provider (CSP) in the world has had no choice but to shift its entire contact center workforce to work-from-home (WFH). In WFH mode, agents tangle with the same complexity and manual procedures they did in the office, now by VPN and home broadband. The day to day complexities agents navigate can be a hurdle for CSPs to achieve the world-class customer experience (CX) they want, especially with WFH as the prevailing model.Here are five common challenges contact center agents can face that CX leaders want to solve:1. Too many systems, yet there are always moreFor any given order, an agent might need to work with separate ordering, provisioning, billing and scheduling systems to run a process or gather status information. As CSPs enter partnerships and make acquisitions, the number of systems can increase as agents navigate partner catalogs and acquired provisioning stacks. Whatever the root cause, agents encounter more systems, rules and procedures to get the job done.2. Measured for efficiency, yet tasked with manual proceduresAgents are typically measured by how fast they resolve issues, how well they connect emotionally with customers, and whether customers feel they’ve had a positive experience. But they navigate manual logins across multiple systems to repeat task-oriented procedures that do not help them move faster or serve customers better.Examples of procedural tasks agents might inherit include:Keying duplicate orders into different systemsMaking account adjustments in the correct billing systemChecking appointment status in a dispatching systemSubmitting a trouble ticket to the right support deskLogging notes and emailing cut-and-paste follow ups to customers3. Now the office is homeMany CSPs moved their contact center agent workforces home in response to the pandemic. Agents face practical challenges as a result like finding a private place to work and being responsible for equipment moved home from the office that IT normally maintains. Agents must maintain their home phone and internet connections to do their jobs too. At the same time, they have to learn new systems, look after customers and try to hit their performance targets.4. More new products and devices are always comingMost CSPs are under pressure to innovate, introduce new offers and sell the latest devices. Anything new needs support, so contact center agents are always exposed to what’s new. Even as CSPs simplify their product catalogs, agents will bear customers’ reactions to change.  Yet agents often must rely on recall and how fast they can look up information in the correct systems to help confused or aggrieved customers struggling with change.As exhibited in the TM Forum Cognitive Contact Center Catalyst, CSPs are now adopting RPA and AI to automate prompts that can help agents manage the deluge of data, communicate with customers and to automate processes like ordering, appointment booking and service recovery.5. Exposed to the battle for customer dataCustomer information is targeted by hackers and identity thieves worldwide. Key CSP processes, like number porting and mobile device unlocking, are attacked in schemes like subscription fraud and digital bank account takeover. Data privacy regulations, like GDPR, have become more stringent as a result, impacting how CSPs worldwide handle customer data and compliance.With thousands of agents working remotely who have access to sensitive customer data, CSPs must evaluate the impacts on risks ranging from insider fraud and misuse of credentials to data privacy violations and network security breaches.One part of the solution may be process automation because it may reduce the number of agents that require direct access to sensitive systems and can monitor sensitive systems and processes with auditable logging and automated alerts.With agents working from home for the long term, issues like these will need to be addressed to fully adapt to WFH and optimize contact center performance and CX improvement.Intelligent automation in the telco contact center Learn more about the changing face of CX in contact centers in this report which includes data analysis and first-hand accounts."
33,"Telco contact centers have been under increasing pressure because of customer experience (CX) transformation, adoption of new technology, mergers and acquisitions, the shift to cloud architecture, and customers’ rising expectations of world-class service. With an expansion in channels to reach customers and major changes in the mix of services offered by communications service providers (CSPs), the contact center agent’s job has never been more complex.Employees working in these channels are managing an expanding product catalog and cross-channel interactions with customers, along with a broader range of care and support scenarios. They are increasingly assisted by maturing tools like chatbots and early uses of AI for prediction and personalization. Many employees have also been part of major business reorganizations that aim to implement customer-centric operations and reward Net Promoter Score (NPS) improvements.In addition to leveraging data from our CX survey, this report includes first-hand accounts from CX, IT and contact center leaders from nine CSPs about the changing role of the contact center in CX. It also validates market trends and lays out the pros and cons of new technology like AI and robotic process automation (RPA).Read it to understand:How the CX environment, the agent’s role and tools are changingHow CSPs are shifting to a WFH model for call center agents as a result of Covid-19What the needs are of a remote agent workforceHow CSPs are using AI, RPA and chatbots to assist agents and do more for customersHow CX measurement is shifting from traditional quantitative metrics to a focus on outcome-based metrics"
34,"TM Forum members have published a new white paper about the use of AI in telco operations, also known as AIOps. This summary explains why communications service providers (CSPs) are counting on it to increase agility, and why collaboration and industry-agreement on an AI-based software architecture is necessary.  AI is going to have a profound effect on communication services providers’ (CSPs’) businesses. It will transform their networks, IT and service operations, enabling them to deliver new, complex services across the digital ecosystem. AI will help them add the agility, speed of service delivery and reliability needed to compete, coexist, and even partner with over-the-top (OTT) service providers, hyperscale cloud providers and other new, nimble digital players – and it will do so while delivering massive cost savings through elimination of manual processes.The McKinsey Global Institute estimates that AI could contribute an additional 1.2% to annual growth in gross domestic product for at least the next decade, which amounts to over $13 trillion of economic activity by 2030. This, coupled with Bain & Company’s prediction that 5G could be worth over $400 billion to CSPs in the B2B2x marketplace, means that operators will be well placed to grow revenue exponentially, which hasn’t happened since the early 2000s.5G is a driverNew business models enabled by 5G and AI are not the only key drivers for cognitive and autonomous network deployment. The World Economic Forum estimates that AI could save CSPs a massive $46 billion in customer acquisition costs and lost revenue through network performance, and deliver a 30% reduction in mobile infrastructure spending by using it for better network planning.Aside from the economic benefits, technological advancements outside of AI are making its deployment a must. The advent of new wireless technologies such as 5G have the potential to add even more complexity to the network, particularly in radio access network (RAN) operations. 5G will make the RAN more complex as it needs forests of tiny antennas to exploit the very high frequency bands (millimeter waves) it uses.In addition, it is estimated that by 2025 there will be a total of 100 billion device connections around the world, which will put a huge amount of pressure on networks. More devices mean more data running across operators’ networks, and IDC forecasts that by 2025 this data will grow by 10 times to reach 175 zettabytes (1 zettabyte equals a trillion gigabytes).Zero-touch is the goalAs devices proliferate and the IoT grows, network and service management must be zero-touch because it is not feasible for manual processes to support the volume and velocity of changes that must happen across the network. A network servicing 10 million end points and 10,000 nodes could see these numbers increase by up to five times, which in terms of incidents per hour could lead to a 25-fold increase from 400 incidents per hour to as many as 10,000.This is impossible to handle manually. CSPs must deploy AI and automation in their networks to manage the coming increase in traffic, but large-scale deployments of AI in operators’ networks creates huge operational challenges such as how to govern, deploy, operate, control and maintain hundreds or thousands of AI models and components which will eventually form part of their core IT and network systems architecture.Unlike traditional software, AI software learns and evolves autonomously when exposed to new input data. AI models are “black boxes” which are potentially even more fragile. They are exposed to bias and are nondeterministic by nature.Addressing the challengesTM Forum members are addressing these challenges through an initiative called AIOps Service Management, the goal of which is to develop an industry-agreed framework that re-engineers the processes involved in the lifecycle of software and service operations management to handle and govern AI software at scale. This will enable operations teams, process owners and business users to exploit AI safely and properly, maximizing its benefits, mitigating risks and ensuring the appropriate level of network and service quality.The AIOps Service Management Framework is applicable to any type of architecture due to its agnostic design, which means it can operate as an independent process framework to manage the deployment of AI in current and future architectures. It is part of TM Forum’s Open Digital Framework (ODF), which includes the Open Digital Architecture (ODA), an open, modern, software-based target architecture that enables new operating and business models fit for the 5G era.The ODA sets out an industry-agreed vision of targeted software and services. It is loosely coupled, cloud native, driven by data and AI, and is made up of standard components which can be easily procured and deployed, without the need for customization. To learn more about the ODA and TM Forum’s vision the future of the software market, download this white paper.To learn more about how to get involved in the Forum’s work on AIOps, please contact me directly, and download the new white paper below.Download AIOps white paper"
35,"Making data a board-level priority is only going to be possible if telecoms operators can demonstrate the value it can bring to the business. To do this they need to show that leveraging data holds the key to delivering an excellent customer experience which in turn, delivers more customers and more revenues.However, executing on a data analytics strategy can be difficult. Creating a ‘single source of truth’ for all this data becomes ever more difficult because of the sheer volume of data that gets loaded onto centralized systems. Operators need to shift their data into the cloud and automate the processes that convert raw data into data insights.Join us for this webinar to understand:Why CSPs’ data strategies are failingHow to use data orchestration to break down silos and turn your data into actionable intelligenceThe path from telco to digital technology playerA telco case study on leveraging data and measuring the business impact"
36,"Data is a powerful weapon in the global fight against Covid-19. A new proof of concept Catalyst project shows how many sources of data can be combined and analyzed on a centralized platform as a service (PaaS) to protect the public, individuals’ health and to ensure buildings are safe. The platform can also provide tools and data as services to help develop applications for internal and external use by very different organizations. The Covid-19 outbreak is stress-testing cities’ emergency response abilities and is a fast-moving, dynamic situation. Big data collected and aggregated from many domains is crucial to enable governments and public bodies to make informed, effective decisions to save lives and halt the spread of the disease.However, most public bodies hold data in various, often incompatible formats, and in systems that have little or no interoperability. In addition to these technical difficulties, some parties are unwilling to share data because they want to protect the vested interests of stakeholders. Other don’t dare share because they lack the right levels of data security and oversight, while others cannot because they still they lack suitable channels.Trust is the keyThe Catalyst project, Trusted data-sharing for smart emergency management (Covid-19), addresses this challenge and the solution was demonstrated at the TM Forum Catalyst Digital Showcase on July 16.It is championed by communications service providers (CSPs) China Mobile, China Telecom and China Unicom, supported by participants Asiainfo, Huawei, SI-TECH and Tianyuan DIC. The three operators draw on knowledge acquired from previous Catalyst projects – see the recommended videos at the end of the article for more information.Putting it all togetherTo build the standard data platform for the current Catalyst, the team drew on TM Forum’s Information Framework (also known as SID) and Application Framework (known as TAM). The platform also uses edge computing, blockchain, data fabric (which pulls many types of data from multiple sources) and secure multi-party computing to support collaboration between all the stakeholders. The platform provides real-time, global data to support governments’ efforts to prevent and control the pandemic, as well as to deliver public information for citizens to reassure them and help keep them safe.Jun Zhu, Chief Architect and Senior Product Director at AsiaInfo, explains that the Catalyst demonstrates a scenario whereby whenever a case of coronavirus is identified and reported by urban hospitals, the platform allows the Centers for Disease Control and Prevention (CDC) to use CSP data to analyze where patients have been and with whom they have been in close proximity. This enables authorities to trace those at-risk contacts so that they can be advised to self-isolate and curb the spread of the virus.He adds, “CDCs can also aggregate data from the police and transportation companies to analyze and manage the flow of crowds and logistics to minimize virus transmission. Heat maps based on telcos’ data can provide real-time monitoring to understand how people are moving around and to stop crowds forming”.The big data platform can also target support for vulnerable people like children, the elderly and those with disabilities and underlying conditions.How does it work?The data circulation mechanism is built on blockchain so data owners can embed their rules for whom is authorized to use it, in what circumstances and for which purposes. Hence blockchain ensures data sharing is legitimate, controllable and regulated, with an immutable audit trail which records every transaction in which the data is involved.  The team drew on the TM Forum Trust, Security & Privacy Toolkit, which is based on blockchain technology.Once data rights are blockchain protected, stakeholders can either upload it to the data virtualization center, or download the secured computation model from the privacy computing center.Data virtualization is based on data fabric technology which pulls data together from various sources then aggregates it to build a unified data model for consumers. In contrast, the privacy computing center uses secure multi-party computation technology so stakeholders can process their data locally.Edge computing and data governance ensure the data’s quality meets the requirements for data sharing in real time and so is trusted by those who consume it.The platform provides the city, government, companies and even individuals with a one-stop service for the data and information they need to better understand, analyze and predict situations. The TM Forum Data Analytics Toolkit was used to develop the data driven application to manage the public health emergency.A silver data lining for citiesThis dark cloud of the pandemic potentially has a silver lining too: The Catalyst demonstrates how these same key technologies deployed to combat the coronavirus outbreak could speed up digital transformation for cities through trusted data sharing. Jun Zhu says, “The team shows how the platform can be useful for managing not only the immediate emergency and its lasting impacts, but also for addressing future challenges and opportunities”.Now available on demand:Watch the Catalyst team showcase their projectWatch the video about the award-winning Phoenix Tree: Centralized big data PaaS platform, which was recognized for its outstanding use of TM Forum assets. It is built to manage 25,000 servers, store 100 Pbs data and support 100 tenants.This video is about the Telco Big Data Security and Privacy Management Framework Catalyst.China telecom and China Unicom championed the Blockchain-based 5G collaboration economy phase III last year,  find out more in this video."
37,"In this report we revisit data analytics but in a broader context. Rather than focusing specifically on the challenges CSPs are facing, we assess the business implications of failing to use data insights to build a better customer experience. We also explore why the telecommunications industry has been slow to draw a correlation between providing better customer experience and improving financial results for shareholders.  As part of our research we interviewed C-level executives and senior VPs from CSPs in multiple regions of the world. They are executives who passionately believe in the power of data insights to unlock hidden value in their businesses, but they expressed disappointment and frustration at the slow, stuttering progress their companies are making.Whatever the reason for previous mistakes or lack of prioritization, the bigger question is: How can CSPs make up for lost time when they’ve already been on multi-year journeys to deliver better data insights? Making this a board-level priority by joining the dots between insights, customer experience and the profitability of the business is essential. But this is just the start.Read this report to understand:Why CSPs’ data strategies are failingHow Amazon links customer experience to financial performanceWhy digital experience is necessary but also challengingWhere to look for the right skillsHow Vodafone is quantifying the business impact of better customer experienceWhat data orchestration is and why it’s importantWhy it’s important to have a board level champion for a data strategyHow Reliance Jio, Celcom Axiata Berhad and PCCW are leveraging dataHow The TM Forum Digital Maturity Model can help"
38,"Who: China UnicomWhat: Development of a novel customer experience measurement and management platform collecting and analyzing massive data volumes across the entire customer lifecycle and customer base to drive specific, measurable business benefits continuouslyHow: Built using TM Forum’s Open APIs, part of the Open Digital Framework, and the Customer Experience Management Lifecycle ModelResults: System costs reduced by US$14.3 million per yearMobile internet NPS improved by 7 points YoY, broadband NPS improved by 13.8 points YoYChurn rate reduced by 36%The Chinese market is extremely competitive and largely saturated with domestic operators. As a result, customer satisfaction and brand reputation are as paramount as efficiency and continuous operating cost reductions. In this setting, China Unicom has made elevating its customer experience a central focus. The company implemented a highly sophisticated, end-to-end customer perception management solution that spans the entire customer lifecycle. The solution makes real-time customer experience evaluations based on NPS, customer satisfaction and other measures like service performance to score every customer interaction and relationship. China Unicom is second largest mobile operator in China and the world’s sixth largest with more than 317 million subscribers. As the company took on a major transformation initiative, its leadership realized it needed to enhance its ability to monitor customer experiences through big data; characterize customers into manageable subgroups; and create customer portraits based on key characteristics, product features, business features, consumption features, and customer evaluation, such as surveys. As Fan Yunjun, Senior Vice President and Deputy General Manager, explains, China Unicom’s aim is to “promote the transformation of the company’s model, thus realizing digital experience, marketing and products, and promoting the overall improvement of Unicom’s capabilities, so as to continuously improve customer perception.”The company aimed to enhance its reputation and customer loyalty as a result by addressing clearly identified needs across specific population clusters and delivering differentiated services for different customer groups.A platform based on large scale measurement, sophisticated data science and AIChina Unicom Headquarters has many branches. In the past, customer surveys were conducted by branch offices on their own initiative, typically by telephone outreach, but there was no centralized coordination. As a result, many of the evaluation functions and capabilities were repeated across branches and hence were redundant and disconnected.To unify this effort, reduce costs and improve results, China Unicom developed its user perception analysis and evaluation system. It is a customer experience management solution for digital perception analysis and evaluation based on big data, AI and other cutting-edge technologies. Based on eight scenarios in the customer lifecycle, the platform performs end-to-end analyses of customer experiences with functions, processes, interactions and scenarios. It drives network, business and service through big data to improve customer perception continuously.China Unicom’s approach to measurement and data collection is robust and diverse. For example, customers are contacted by telephone to conduct a visiting survey. For app users, survey entries are placed at the bottom of the business processing page so that interested customers can click to participate. Customers who follow China Unicom’s official, public WeChat account are invited to participate in surveys through a push link. Other customers are invited via text with a push link that leads to a survey webpage. Customers in Unicom shops are pushed a link within three minutes of transacting business so that a salesperson can encourage participation on the spot. The company has also established a dedicated WeChat public account through which internal users and customers are invited to participate in specific product experience surveys. Participants are granted redeemable loyalty points as a reward.Customer feedback is categorized and distributed to the appropriate department. Responsibilities are assigned to each professional and post so that each business unit can improve service capability quickly. Concurrently, in accordance with each customer’s evaluation result, the solution projects potential customer needs and depicts a perceptual portrait of the customer in order to drive personalized service.The customer perception management platform program was launched in 2018 with basic NPS and public opinion modules, but radically expanded in 2019. China Unicom was able to establish a unified, end-to-end customer experience operation and real-time evaluation system including end-to-end customer experience measurement and intelligent, voice quality NPS inspection. This not only unified management and evaluation, but also resulted in substantial cost savings regarding system construction and research costs.TM Forum leads the wayChina Unicom’s  customer perception measurement  platform uses the TM Forum Customer Management API and is based on the Forum’s Customer Experience Management Lifecycle Model,  which allows us  to define customer interactions  into the various phases and subphases and model those based on their unique journeys, as well as a three-layer optimization model. This model covers preventive measures based on NPS, customer satisfaction evaluation, and effective management of complaints to prevent and control risks. The model is leveraged to refine user behavior in each stage and to control how the system intervenes or participates in or impacts the user experience.Customer perception data has been integrated in multiple fields. Automatic speech recognition (ASR) was introduced to achieve intelligent voice quality inspection based on NPS. ASR greatly reduces the workload and cost associated with manual quality inspection of telephone recordings and surveys. ASR transforms voice recordings into text files which improves the efficiency and accuracy of analyses.The company built 11 public opinion data models to fully retrieve acquired information and analyze public opinion in terms of theme evolution, time trend, topic communication, and more. This enables comprehensive customer experience evaluation with emotional connection scores (ECS). ECS is a comprehensive calculation and includes analysis or various modules including NPS, real-time experience evaluation, public opinion, complaints and other modules to explore the reasons for custom dissatisfaction.Reaping the benefitsChina Unicom’s approach has greatly enhanced customer satisfaction and Unicom’s brand reputation. Though customer participation was low initially, it grew over time as brand perception improved. Similarly, customer propensity to cancel or disconnect service was significantly reduced as NPS and ECS scores continuously improved. Both customer retention and revenue retention increased.The scope of this effort is notable. Layered mathematical models and random sampling methods are used to achieve multi-point control of 490 million user samples as well as complete NPS assessment of more than 1 million users per year.The platform provides portrait services for each incoming call, matching more than 300 models for each call on average, with more than 10 billion markings per month and more than 1 million business opportunity data outputs shipped to marketing.Even with this scale, the new platform reduced user assessment costs by RMB 1.48 million (US$212,000); process management costs by RMB 1.1 million (US$160,000); and system construction investment by about RMB 100 million (US$14.3 million) in 2018.Relatedly, mobile internet NPS increased by 7.0 points year-on-year and broadband NPS increased by 13.8 points year-on-year in 2018.China Unicom also built end-to-end self-service modeling tools by integrating Word2vec, fastText, K-means, THUCTC and other text analysis and processing algorithms, which helped business units build more than 8,000 text labels. Using natural language processing algorithms, the company performs phonetic transcription for 50 million customer consultation and complaint calls per month and conducts customer portrait analyses for consultation and complaint work orders.End-to-end satisfaction evaluation has been achieved for 14 key scenarios in the customer life cycle, benchmarked against 46 customer-service indicators, and with customer satisfaction as a measure of service implementation, thus providing a “barometer” of customer perception evaluation in all channels.ECS perception evaluation is conducted for 4.86 million users per month based on six categories of indicators including service packages, call quality, Internet quality, complaint perception, user value and customer evaluation. More than half a million at-risk customers per month are saved based on this practice, reducing China Unicom’s churn rate by 36% and improving customer satisfaction by 13%.Maximizing impactTo create this program, China Unicom had to develop and refine a variety of techniques in customer experience measurement and management and to determine which had the most direct impact on its operating results. The company learned many lessons as a result which helped it to realize  specific customer experience, cost and revenue benefits.The team created a digital customer emotional experience module. It was optimized through multiple iterations to refine how public opinion and perception data is acquired and used. The team also learned to use its NPS-based customer evaluation system to identify the most valuable customer data and to target the correct customer mix for perception surveys. Its satisfaction questionnaire and multi-scenario optimization solution determined ways to improve customer experience perception further.China Unicom learned to measure and manage customer experience perception with the integration of both business and network domains to improve customer experiences in specific vertical industries. In turn, the company says it will be able to  use this  approach  to improve its 5G network as well as its operational and maintenance efficiencies to reduce costs and continuously improve customer perception in the future. China Unicom has been nominated for TM Forum's Excellence Awards 2020 in the Customer Experience &  Trust category. Click the logo to see a full list of award nominees."
39,"Who: Celcom Axiata BerhadWhat: Enterprise data governance program implementationHow: Built strategic and operational components of data governance, engaged key employees to activate programs across the organisation, and used the TM Forum Big Data Analytics Maturity Model to baseline and measure progressResults: Increased data governance score in maturity assessment from 30% in January 2019 to 95% in November 2019Established common strategic frameworks and standard operational workstreams where scattered data governance practices had previously existedTrained 52 data governance champions from all parts of the organisation, defining four data roles and identifying individuals to fill themEnhanced capability to protect and leverage data, reducing liabilities while expanding the potential to drive value through better analyticsCelcom Axiata Berhad (Celcom) is one of six operating companies within the Southeast Asian telecoms conglomerate Axiata Group, which provides telecommunications services across 10 Asian markets and has become a champion of digital transformation in the region. Celcom serves almost 13 million users across Malaysia.Celcom launched its data governance program in 2018 after having built a centralized data lake for the collection and use of data across its business. High-level objectives of the governance program included ensuring data availability, usability, integrity and security. The business goals included the development of a single source of truth, developing descriptive, predictive and prescriptive analytics models as well as risk mitigation purposes, i.e. the reduction of duplicated data across the organization.Celcom’s Sunny NiralaSunny Nirala, who was Tribe Master and led Celcom’s Cross Functional Team (CFT) on Enterprise Data Governance (EDG) said, “The primary objective was about protecting the data of the people we serve. In a world where data breaches are happening, we wanted to be in a place where we could reassure customers, and of course avoid getting fines, but also knew we needed to have a practice around this that would put us on the path of excellence.”Strategic and operational teamsCelcom’s CFT on EDG focused on two broad areas: Strategy and Operations. The overall team was comprised of 52 individuals, broken into eight squads that worked within scrums on three strategic and five operational pillars of data governance.The three strategic teams focused on building long-term frameworks, ensuring that an overarching operating model, data privacy and protection, and data quality policies were in place. The operating model team addressed questions such as what a data governance organization would look like. It leaned on the DAMA Data Management Book of Knowledge (DMBoK), which the Axiata Group company had endorsed. The data privacy and protection team looked at the applicability of certain models, such as the EU’s Global Data Protection Regulation (GDPR). The data quality team addressed how to keep data accurate, consistent and usable over time.The operational teams addressed five components and were reviewed on a periodic basis given changing circumstances:Roles and assignments. The roles include data stewards (who ensured that data is correct), the data owner (who holds accountability), data custodians (who ran the systems) and data citizens (who used the data). Assigning particular employees to fill these roles helped in bringing this work stream to life.Book of Definitions. Similar terms may have different meanings, which can impede progress; the effective use of data requires a common and current language.RACI. The responsible, accountable, consulted and informed (RACI) model for linking processes with offices or departments during an organisational change.KPIs. Which metrics matter most in data governance? They could involve adherence to policy, customer experience, risk reduction, data quality, etc.Tools. Software tools to help improve data quality, automate previously manual activities and address other issues, such as automated data lineage, data profiling, stewardship and more.The output of this program was not only the eight documents addressing these topics, but also the approach taken, in particular the team’s engagement in scrums, sprints and daily standups. The overall effort was framed and influenced by TM Forum’s Big Data Analytics Maturity Model, which relies on TM Forum best practices and includes governance as one of its ten operational areas. The completion of the program and its transition into business as usual (BAU) operations was marked by a “Data Festival” event which hosted a number of external speakers along with various stands related to cyber security, data management, and data related entertainment.Moving forward, Celcom has operationalized the newly set up Enterprise Data Governance department, under the Strategy & Value Creation Office.Program resultsThe most significant metric associated with this program resulted from the TM Forum Big Data Analytics Maturity Model, which is a precursor to the Data dimension of the TM Forum Digital Maturity Model (DMM). An external assessor set a baseline in January 2019 and then repeated the test in November 2019 to measure progress. Over that ten-month period, Celcom’s data governance score rose from 30% to 95%, indicating a widespread and significant growth in relevant documentation and actual adoption.Several factors contributed to the dramatic increase in Celcom’s data governance score:Although data governance had existed in a few pockets across the organization prior to this initiative, the new frameworks and work streams set standards and company-wide expectations.Participation within the eight scrum teams indirectly trained participating employees to become data champions in their respective departments.By taking better care of data, Celcom was positioned to boost confidence amongst customers and internal teams, driving an increased value into the business from both ends.Another result has been to advance Axiata Group’s goal of becoming a digital leader. “Data really is at the center of the company now,” said Nirala, who also led and managed the data lake program in 2018. “It’s quite humbling to see something start from zero to where now hundreds of people rely on it.” Digital transformation may be the target; but getting there requires hitting many intermediate goals.Lessons learned and next stepsCelcom learned that changing a fundamental business activity, such as its use of data, requires continuous and ongoing engagement with a wide range of employees. Linking new processes with individuals is another way to overcome inertia and deliver a more lasting change. As for technology, it can both disrupt and simplify, especially where automation is involved. Framing such deployments can accelerate its adoption.Meanwhile, the digital mission of Celcom continues. After creating a data lake and building a data governance infrastructure, the third phase of its digital transformation involves enhancing data quality. Celcom has already laid a solid foundation for achieving its next goal.  Celcom Axiata has been nominated for TM Forum's Excellence Awards 2020 in the AI, Data & Analytics category. Click the logo to see a full list of award nominees. "
40,"Who: China UnicomWhat: Needed to reduce the resources required for billing and find a future-proof solution for billing and its associated systems ready for the bigger data surge from 5GHow: Use of cloud native, artificial intelligence platform to predict usage, reducing the number of CCR messages by 63% and CDRs by 52%, leveraging TM Forum’s Open APIsResults: Savings of 50% on bandwidth and computing resources, as well as more than halving the amount of storage requiredSince the launch of 4G, operators worldwide have seen explosive growth in traffic as data limits have risen, prices have fallen, and the networks’ capacity and speed increased. This was China Unicom’s experience, accompanied by soaring number of messages concerning credit control requests (CCRs), generated by the Diameter Credit-Control application protocol which defines a charging mechanism for pre-paid users. To do this, it uses a credit-limit control to implement session- and event-based charging. Likewise, the rise in traffic also produces huge numbers of call detail records (CDRs).Some idea of the scale of the problem is shown by the fact that in Shandong province alone, China Unicom found the number of CCR messages rose by 126% in less than a year, with the average daily volume of them reaching 1.57 billion. This put immense strain on network elements, storage and billing, and the many systems from which billing takes information.Traditional billing solutions cannot handle the problems caused by this steep rise in 4G traffic as they are typically designed to deal with a fixed quota, such as 30MB for 30 minutes. In addition, the rapid advance of 5G services brings an unprecedented surge in traffic and message volume, putting huge pressure on computing and storage resources. More investment and energy consumption are needed.Taking the strainThere is an urgent need to reduce the strain on and soaring costs of storage, bandwidth and systems associated with billing. At the same time, operators need to improve operational efficiency as well as to implement global governance strategies and invest heavily in new hardware and especially software technologies – and all with without negatively affecting users’ experience.Against this backdrop, China Unicom decided to explore how artificial intelligence (AI) could be used to address these challenges. In parallel it wanted to investigate how edge computing could reduce the pressure on the network elements across the infrastructure and their associated systems, as well as help with the expected burst of traffic at the edge.Using AI to build modelsAs a first step, the operator used big data analysis to determine the most effective way to train the AI. The AI monitors how the models run, then ‘learns’ from outcomes, and through continuous iteration of the models optimizes their performance.From the tags, the models predict data about users, services and time periods, and assigns an intelligent data quota, which can be matched against actual usage in real-time. This predictive capability reduced CCR-related messages by 63% in Shandong province, which equates to 964 million fewer CCR messages every day (also see results below).By the end of the trial, China Unicom had cut, by 50%, the bandwidth and computing resources needed to handle the billing and CDRs efficiently. This massively reduced the amount of processing and response delays from network elements, the online charging systems and the business service inquiry system, such as for balance and account inquiries, which is related to 5G billing, which were not part of the trial. All of which also meant that customers noticed that the service had improved.Other systems that are connected to the 5G billing system, like big data platforms, will also save a lot of storage space and improve the overall efficiency of the systems’ processing.AI is the keyWhile the data analytics are used to mine the data held in core billing systems, and to identify that value through business operations, it is the AI which realizes that value. The AI-powered intelligent billing in this project makes a strong use case for operators to employ AI technology to empower their cloud-based systems.This project also covered edge computing. By dynamically allocating traffic quotas based on users’ online behaviour, the amount of data is reduced significantly. This means that after data processing at the edge, only the useful data is sent to the billing system.In future, new use cases will be explored such as: automatic switching of online and offline charging based on the user’s credit, balance and other information; and intelligent control based on the user’s 5G context, such as low-latency scenarios. China Unicom will also consider putting part of its computing capabilities on the edge of the network.ResultsDaily CCR messages reduced by 964 million equating to a 63% fall from 1.57 billion, which reduced the monthly total of CCRs to 28.917 billion (30 days x 796 million).The daily number of CDRs fell by 52%, to 468 million from 900 million, which reduced the monthly total to 14.040 billion (30 days x 468 million)Average amount of storage needed daily before going online was 3.86TB, which fell by 1.84TB when operations moved to the containerized platformThis reduced monthly storage by 55.2TB (30 days x 1.84TB)The system stores 13 months’ of data, hence over that period the amount of storage required has fallen by 717.7TB (13 months x 55.2TB)As China is the world’s third largest country in terms of landmass and has the largest population, the success of this project suggests if this platform and approach can solve the challenge in China, then they can be implemented anywhere on the planet by operators to address the same issues.  China Unicom, Whalecloud & Si-tech have been nominated for TM Forum's Excellence Awards 2020 in the AI, Data & Analytics category. Click the logo to see a full list of award nominees."
41,"The need for analytics in telco networks can be traced back to the early days of public switched telephone network (PSTN) and signaling system no. 7 (SS7).Simple network-node key performance indicators (KPIs) were used based on stats received from T1/E1 link probes for network monitoring and to ensure no fraudulent activity was happening on the network alon with network performance monitoring.  As part of the 4G evolution, analytics using advanced AI/machine learning (ML) algorithms doing predictive analytics, anomaly detection, trend analysis, and clustering have become a top choice for use cases such as customer experience management, personalized marketing or data monetization in addition to network management. With the addition of 5G and its complexities, there’s an even stronger need for advanced prescriptive analytics to drive closed-loop automation and self-healing networks.With all of these considerations, there’s a common pain point among communications service providers (CSPs) – how to integrate analytics into the network, currently the analytics are complex from the many non-standardized interfaces and inconsistent data collection techniques across network vendors.The good news – these concerns may finally be addressed by the network data analytics function (NWDAF) defined as part of the 5G Core (5GC) architecture by 3GPP  (the standards development body for mobile networks).  So what exactly is NWDAF? What are the use cases? What will be the key challenges for CSPs? How can they overcome them? Surprisingly little has been written about NWDAF but we’re hearing a lot of interest in it from CSPs around the world.What is NWDAF? – Architecture, services and use casesNWDAF defined in 3GPP TS 29.520 incorporates standard interfaces from the service-based architecture to collect data by subscription or request model from other network functions and similar procedures. This is to deliver analytics functions in the network for automation or reporting, solving major custom interface or format challenges.NWDAF is expected to have a distributed architecture providing analytics at the edge in real-time and, a central function for analytics which need central aggregation (e.g., service experience).NWDAF collects data and provides analytics services using a request or subscription model as outlined in the example below.3GPP TR 23.791 has currently listed the following formula-based/AI-ML analytics use cases for 5G using NWDAF:Load-level computation and prediction for a network slice instanceService experience computation and prediction for an application/UE groupLoad analytics information and prediction for a specific NFNetwork load performance computation and future load predictionUE Expected behaviour predictionUE Abnormal behavior/anomaly detectionUE Mobility-related information and predictionUE Communication pattern predictionCongestion information – current and predicted for a specific locationQuality of service (QoS) sustainability whic involves reporting and predicting QoS changeNWDAF deployment challenges and recommendationsTo deploy NWDAF, CSPs may encounter these challenges:Some network function vendors may not be standards compliant or have interfaces to provide data or receive analytics services.Integrating NWDAF with existing analytics applications until a 4G network is deployed is crucial as aggregated network data is needed to make decisions for centralized analytics use cases.Many CSPs have different analytics nodes deployed for various use cases like revenue assurance, subscriber/marketing analytics and subscriber experience/network management. Making these all integrated into one analytics node also serving NWDAF use cases is key to deriving better insights and value out of network data.Ensuring the analytics function deployed is integrated to derive value (e.g., with orchestrator for network automation, BI tools/any UI/email/notification apps for reporting).Here are some ways you can overcome these challenges and deploy efficient next-generation analytics with NWDAF:Mandate a distributed architecture for analytics too, this reduces network bandwidth overhead due to analytics and helps real-time use cases by design.Ensure RFPs and your chosen vendors for network functions have, or plan to have, NWDAF support for collecting and receiving analytics services.Look for carrier-grade analytics solutions with five nines SLAs.Choose modular analytics systems that can accommodate multiple use cases including NWDAF as apps and support quick development.Resource-efficient solutions are critical for on-premise or cloud as they can decrease expenses considerably.Storage comes with a cost, store more processed smart data and not more raw big data unless mandated by law.In designing an analytics use case, get opinions from both telco and analytics experts, or ideally an expert in both, as they are viewed from different worlds and are evolving a lot.The bottom lineCSPs are familiar with the benefits of using analytics in telco networks, which include reducing operations and capital costs and generating new revenue.  As they move to 5G, analytics play an even bigger role beyond the traditional boundaries of telco networks including radio access networks (RAN), and core operations/business support systems (OSS/BSS).  Hence, having a standards-defined NWDAF for the analytics needs of 5G, deployed with the right scalable, optimized and distributed architecture, will simplify 5G/hybrid network deployment and management and is critical to ensuring the very best customer experience."
42,"Who: Telekom Malaysia Berhad and TM R&DWhat: Developed Intelligent Network Diagnostic & Expert Advisory System for Service Desk (IDEAS+) to help agents quickly resolve customer problemsHow: Using TM Forum’s AI Maturity Model, part of the Digital Maturity ModelResults: Quarterly savings of over RM780,000 (US$190,000), up to 3x faster average handling time and troubleshooting accuracy increased to 98%How important is great customer service? In a recent survey, bad customer service was the ‘number 1’ reason why 39% of people canceled a contract. In fact, just a single negative experience—such as having to sit on hold—was enough for 18% to churn.Telekom Malaysia Berhad shows how operators can fix the problems that frustrate customers, including slashing contact center queue times by up to one third and increasing service desk troubleshooting accuracy to 98%. In the process, Telekom Malaysia is saving over RM780,000 (about US$190,000) each quarter  through fewer truck rolls, and greater efficiency and productivity by contact center and service desk staff.Telekom Malaysia didn’t have to look far for help. It partnered with Telekom Malaysia Research and Development (TM R&D), an innovation hub of the Telekom Malaysia group, on Intelligent Network Diagnostic & Expert Advisory System for Service Desk (IDEAS+). Using technologies and practices such as machine learning and DevOps, the IDEAS+ platform supports Telekom Malaysia’s unifi and pre-unifi fixed broadband and mobile services for consumers and enterprises.“It provides an effective way to diagnose problems, offers simplified ‘next best action advisory’ to all front liners and also recommends second-level escalation to the respective units, if necessary,” says Mohd Khairi Mohd Yunus, Senior Experience Architect, Telekom Malaysia Berhad. “This tool really helps all front liners solve unifi problems faster and more efficiently.”Machine learning helps agents work smarter and fasterThe IDEAS+ project launched in August 2018 with four objectives:Reduce average handling time (AHT) for customer callsReduce repeat truck rolls by 2,000 per monthIncrease the accuracy of troubleshooting and escalationReduce costs by RM3,300,000 (about US$771,000) by Q1 2020The machine learning in IDEAS+ uses two pattern-recognition algorithms. One is k-nearest neighbors, which uses existing, known variables to understand a new, unknown variable. The other is naïve Bayes, which creates a hypothesis or assumption about a variable based on information it already has. For example, knowing that most people don’t like cold rain, naïve Bayes could look at the current weather and assume that most will stay indoors. These algorithms help IDEAS+ quickly analyze the myriad variables that Telekom Malaysia agents face each day as they field customer inquiries.“IDEAS+ has more than 500 classifications,” Khairi says. “It federates the data, metadata and multimedia from various Telekom Malaysia data sources, which come from passive and active network equipment, network performance and alarm systems, customers, billing and customer applications.”The human brain uses experience to do similar analysis, but IDEAS+ can do it much faster. Now agents are presented with the information they need to resolve a problem faster than if they had to do the bulk of that troubleshooting analysis on their own. This streamlining improves customer satisfaction by reducing the time they spend on hold and speaking with an agent, and by ensuring that the agent selects the right solution, such as dispatching a technician trained for that particular problem.Hold times and diagnostic times plummetThe initial IDEAS+ rollout in March 2019 covered 41 service desk agents. By August, it was expanded to 522 agents, along with 82 TMPoint retail outlets.“In 2018, AHT averaged 516 seconds, with 240 seconds used by the agents to troubleshoot, diagnose the issues and provide resolution,” Khairi says. “IDEAS+ takes 30-40 seconds to pull data from multiple sources, and another 1-2 seconds for its machine learning algorithm to analyze that data and complete its diagnostics. So we were able to reduce those 240 seconds into an average of 35.70 seconds.”“Based on observation from April to July 2019, 42% of agents completed the same job in less than 20 seconds. Only 2% required more than 120 seconds.”The IDEAS+ platform received a software upgrade on December 3, 2019, further improving resolution times.“Over 61% took less than 40 seconds to solve complaints, and none above 80 seconds,” Khairi says.The platform’s machine learning algorithm played a key role by making the troubleshooting process less reliant on each agent’s experience. So in addition to making the process faster, machine learning also improved its accuracy, which improves customer satisfaction and Telekom Malaysia’s bottom line.Here’s how: Before IDEAS+, the service desk had a troubleshooting accuracy rate of less than 60%. When agents were wrong, they dispatched technicians who, over 95% of the time, didn’t have the right expertise to fix that type of problem.“By using IDEAS+, we are able to improve the diagnostic accuracy to 97.59%,” Khairi says.By virtually eliminating the waste of truck rolls with the wrong technicians, and by maximizing each agent’s productivity through machine learning, IDEAS+ is currently saving over RM780,000 (around US$190,000) each quarter . TM expects additional savings from being able to reduce man hours by 75% as each agent can now handle more customers.Machine learning also helps new agents get up to speed quickly by providing diagnostics that used to require extensive hands-on experience.Telekom Malaysia’s Mohd Khairi Mohd Yunus“Before IDEAS+, the learning curve to train agents was costly because it required 26 days due to all of the content and scenarios,” Khairi says. “Now we train them only on IDEAS+, which has flattened and shortened the learning curve.”Teamwork and cultural transformation are keyTo successfully implement IDEAS+, Telekom Malaysia’s development and operations teams had to be able to collaborate effectively.“We endorse the scrum-Agile and DevOps framework,” Dr. Zainuridah Yusof, Technical Product Owner, TM R&D  says. “The operations engineers who deploy and operate IDEAS+ have no Agile practices and don’t work at the same pace as the Agile developers. The developers create software or migrate it off as ready-to-go to the operations team. The developers will make sure everything is working as it should.“The operations team, however, sees new algorithms and software as the potential for risk and can slow down the deployment process. When there are any communication challenges between the development and operations teams arise, it will slow down during the deployment processes.TM R&D’s Dr. Zainuridah Yusof“Both teams need to be agile and ready to accept frequent change requests. Both must work together and trust each other so both teams will work faster, efficient and continuously.”The end result was a cultural transformation to an Agile DevOps approach.“This makes both employer and employee flexible, trusted and willing to learn new skills,” Dr. Zainuridah  says. “The employer needs to educate themselves on the Agile framework and DevOps philosophy. Besides that, TM R&D provides DevOps with suitable tools such as Jira, Gitlab and Confluence to achieve every goal.”TM Forum provides a common frameworkA variety of TM Forum assets were used to develop IDEAS+, including the AI Maturity Model, which serves as a guide to how the industry uses AI to realize business value. Aligned with the TM Forum Digital Maturity Model and developed through collaboration between leading players across the telecoms industry, it helps CSPs harness AI’s potential by identifying gaps across six key dimensions of their business:StrategyOperationsCulture, People & OrganizationDataPartyTechnologyTM R&D also drew on some of the Forum’s best practices and guidebooks to help it fully leverage the AI Maturity Model, including:Artificial Intelligence User Stories & Use CasesDigital Services Consumer Workforce TransformationThe CSPs Guide to AI-Driven 360 Degree Customer ProfilesAI Data Training Repository“TM R&D believes that TM Forum assets enable interoperability and collaboration for a common framework,” Ir. Dr. Abdul Aziz  Abdul Rahman, Head of Unit for Data Science, TM R&D  says. “This framework allowed us to develop a future-proof platform so that we can easily integrate any external system and support new requirements as they emerge.”  Telekom Malaysia and TM R&D have been nominated for TM Forum's Excellence Awards 2020 in the Data, AI & Analytics category. Click the logo to see a full list of award nominees."
43,"Who: China Unicom & BONCWhat: Implemented a single, centralized cloud-based analytics platform for all of its 24 departments in the parent group, 31 provincial branches, 358 municipal branches and 26 subsidiariesHow: Redesigned the platform using various elements of TM Forum’s Open Digital Framework, including Open APIs and the Business Process and Application frameworks, as well as real-time data streaming enabled by Apache KafkaResults: Daily operational analysis reports arrive seven hours earlier; production efficiency up 70%; accumulated net profits up 458% in first yearLike most communications service providers (CSPs) around the world, China Unicom – one of China’s ‘big three’ mobile operators – is keen to harness the power of big data analytics to improve efficiencies and gain valuable and actionable insights.However, China Unicom also had a somewhat unique challenge in the sense that it’s not simply a single operator with a single network. China Unicom is a complex organization comprising 24 departments in its parent group, along with 31 provincial branches, 358 municipal branches and 26 subsidiaries, all with huge databases and specific needs for analyzing all that data.The other key challenge is that the size of its operations also meant Unicom was becoming increasingly swamped with data.“With the widespread use of 4G networks, the data has grown explosively,” says Wang Zhijun, Vice General Manager of Information Technology Department, China Unicom. “The scalability, service performance and other aspects of the oldsystem were unable to meet our requirements of business processing efficiency.”Rather than a simple upgrade of its analytics capabilities, Unicom decided to embark on something more ambitious: a centralized analytics platform that could serve the needs of every department, every branch and every subsidiary of the business, which would improve efficiencies across the organization and keep the cost of usage low.In preparation for the project – billed as an “enterprise-level big data analysis platform” – Unicom identified several key challenges that it wanted this centralized platform to address:How to access the latest data and generate daily operational analysis reports as early as possibleHow to aggregate and unify all operation and management data across four key domains: business support systems (BSS), operation support systems (OSS) management support systems and e-commerce systems (which Unicom refers to collectively as the BOME domains)How to respond quickly to analysis requirements via things like artificial intelligence (AI) functions, online self-support capabilities and component-based agile applications.Distributed cloud architectureUnicom started by adopting a distributed cloud architecture, using Apache Spark as the computing engine and a vectorized computing module to enable the platform to crunch massive amounts of data faster and more efficiently.That’s key because as mentioned above, the platform integrates data from across all of the “BOME” domains – that’s a ginormous amount of data, says Wang.China Unicom’s Wang Zhijun“On a daily basis we are now processing data volumes of around 17 Terabytes,” he says. “So the centralized big data analytics platform has to be capable of analyzing billions of records in just seconds. This enables more high-value and empowered decisions for business management, marketing management, business management, network operations and customer perception.”Wang adds that Unicom has achieved this panoramic analysis of cross-domain data via a combination of technologies like big data mining, AI and geographic information systems (GIS).GIS is a key component because Unicom’s organizational structure is not only complex, but spread across China’s vast geography.“With the help of GIS, we have better visualization of large scenes using heat maps,” Wang explains. “It also enables us to combine accurate geographic location with business data for our millions of network base stations. We can also use a spatial search engine to search massive spatial data, which is convenient for accurate analysis and marketing assistance.”While the cloud architecture is distributed, the platform itself is designed to be centralized at an ultra-large-scale, which not only saves on construction costs, but also facilitates the rapid development of cross-domain integration and centralized analysis of big data across the group.“On the platform, all of the group’s data can be viewed, cross-domain data can be analyzed intelligently, abnormal data can be flagged early, information can be accessed efficiently and users can improve their analysis skills through recommendations and the accessible knowledge base,” Wang says.The platform also enables real-time data streaming (using Apache Kafka) to support multi-dimensional real-time data monitoring and analysis for apps such as 5G and number portability. With the page refresh frequency at 1 minute/time, users can observe business management development and off-grid trends as they happen.China Unicom’s enterprise-level big data analysis platform adopts a microservices architecture based on TM Forum’s Open Digital Framework such as the Business Process Framework (also called eTOM) and Application Framework (also called TAM). The data warehouse creates more than 100,000 data tables based on the Information Framework (SID) and meets the Forum’s best practice requirements for data application presentation. The platform’s sharing interfaces use TM Forum Open APIs, including the Forum’s API Design Guidelines and specifically the Service Inventory API, Service Ordering API and Service Problem API.Faster report generationChina Unicom’s platform project took around one year to complete – work began in May 2017, and the platform went live in May 2018. Each and every one of Unicom’s departments, provincial branches and subsidiaries can access the platform, which is currently serving 10,000 PC users with 200,000 page views per month and 8,000 mobile users with 30,000 page views per month.The first and most obvious benefit of the revamped analytics platform is that daily operational analysis reports are available first thing in the morning rather than at the end of the business day.“Previously, for example, we would have to wait until the end of Tuesday to receive the daily operational analysis reportfor Monday, but now because we are processing data more efficiently, we can receive that same report at 9:00am Tuesday, even though we are now processing more data than we were before,” Unicom’s Wang explains.Moreover, he adds, the platform has improved production efficiency by 70%, helping business departments to analyze early, deploy early and act early. “The marketing department and customer service department can carry out more targeted marketing, customer maintenance and other operations, including new customer development, old customer maintenance, existing customer production package upgrade and so on, to enhance our competitiveness and improve profit.”In fact, Unicom credits the new analytics platform with boosting its operating performance a whopping 458% year-on-year to RMB10.2 billion (around US$1.5 billion) in annual accumulated net profits in 2018, and up another 12% year-on-year to RMB9.8 billion (around US$1.4 billion) as of September 2019.The multidimensional self-service reports enabled by Apache Kylin (a big data analytics engine with online analytical processing [OLAP] on Hadoop) has cut the cost of service by 50%, and currently covers 53% of self-service demands of Unicom’s business divisions – which for now comprises monitoring and analysis of key businesses, Wang says.“Because new businesses keep emerging, the statistical caliber is diverse, and the self-service model construction has a certain period, it is impossible to cover current business demands 100%,” he explains. “At present, we are doing our best to improve the self-service demand coverage rate, which is planned to reach 70%, and continue to reduce the cost of manual service.”  China Unicom & BONC have been nominated for TM Forum's Excellence Awards 2020 in the Cloud Native IT & Agility category. Click the logo to see a full list of award nominees."
44,"Europol’s European Cybercrime Centre estimates that global telecommunications fraud costs roughly $33 billion, however the true cost likely exceeds even that grandiose figure. Telecommunications fraud has become a misnomer because thieves attack the interconnectedness among people and industries. With telecommunications services, particularly mobile, providing the means through which nearly all industries serve their customers, thieves are finding and exploiting vulnerabilities with no regard for arbitrary industry boundaries.Fraud is a sprawling subject intertwined with cybersecurity. In this report we look specifically at how thieves abuse digital communications technologies to commit crimes, focusing on some of the latest and most widespread forms of fraud which are impacting customers of communications service providers (CSPs) in an era of digital experience.In addition to interviews with CSP executives and other experts, our research includes quantitative data from three key sources:A targeted survey of fraud managers conducted for this report across TM Forum’s global membershipThe 2019 TM Forum Fraud Management Survey which provides an overview of CSPs’ anti-fraud practicesSurvey data from the report’s sponsor, FICO, relating to consumers’ security preferences at the intersection of the communications and banking industries"
45,"Who: China Unicom, one of China’s ‘big three’ operators, and Whale Cloud, a subsidiary of AlibabaWhat: China Unicom wanted to share data with enterprise customers, complying with security and privacy and without increasing data storage and costsHow: Built a centralized data-sharing platform, based on TM Forum’s Open Digital Framework, to unify data from all data centers across China Unicom Group, including 31 subsidiariesResults: Attracted over 1,000 enterprise customers from over 20 industries to use the data-sharing platform to develop and augment services. China Unicom has also saved around $22 million in IT investment related to data storage and managementBy connecting people, information and objects like never before, 5G opens up huge opportunities for enterprises in a broad range of sectors to do things more efficiently and offer new services powered by data – and for telcos to offer new value to these verticals.However, the true power of this data can only be unleashed if it is broken out of silos.This is why China Unicom built a centralized data-sharing platform with technology partner Whale Cloud. The platform unifies data from all data centers across China Unicom Group, including 31 subsidiaries, and enables the operator to share and trade data with enterprise customers to allow them to develop and augment services.Traditionally data is managed in multiple data repositories from across scattered data silos. China Unicom’s platform is rooted in a ‘One Data, One ID, One Service’ strategy.The platform uses mobile numbers to verify users and allows data to be referenced between systems, rather than replicated. This approach has already saved China Unicom around $22 million in IT investment costs related to data storage and management.Powered by blockchainThe platform is underpinned by decentralized blockchain technology, which also establishes a ‘trust-chain’ between data consumers to producers. This is because users must authorize data transactions, and they are traceable and immutable.Further, the data is desensitized and anonymized via the platform before it is shared.Whale Cloud’s Zhengcang Xiao“Blockchain technology has been adopted to manage data sovereignty, privacy and transparency issues, which to date have been obstacles for data-sharing,” said Zhengcang Xiao, CTO, Whale Cloud International.The platform is already delivering benefits for businesses, consumers and China Unicom. The company is now sharing data with over 1,000 enterprise customers from more than 20 industries, including banking, insurance, technology, e-commerce, aviation, real estate, consulting, handset vendors, government, education, media, manufacturing and tourism.This includes 74 automotive companies responsible for around 15 million connected cars.In a finance context, for example, a user could authorize a financial institution to obtain their credit data from China Unicom’s data-sharing platform to determine loan eligibility.  This speeds up and simplifies the process for consumers, and allows financial institutions to run checks faster and boost customer experience. Meanwhile, China Unicom generates new revenue with a charge per API call.In another scenario, a connected car could obtain a user’s itinerary from the data-sharing platform, such as a concert ticket which has been booked online. The user could then receive a reminder, via their phone, to travel earlier to avoid traffic jams, for instance.In the insurance sector, in the case of a traffic accident, insurers and rescue companies could get a user’s GIS information from the data-sharing platform to help them respond faster.Building the platformThe three-layer platform has an infrastructure-as-a-service (IaaS) layer as its foundation, comprising cloud computing and big data capabilities. The central platform-as-a-service (PaaS) layer incorporates API enablement and access authorization, and includes a unified data center, an AI capability center and a GIS capability center. This API enablement platform, made up of 771 APIs, contains all China Unicom’s big data and AI capabilities, and exposes them to front-end applications. It sees an average of 200 million API calls per day.To support this huge frequency in call requests and ensure high system availability, the API gateway uses a cloud-based microservices framework. A Kubernetes open source container orchestration system controls resource management and scheduling. Flink and Kafka are used for data caching and real-time computingFinally, the top, outward-facing software-as-a-service (SaaS) tier makes data available to external industries.China Unicom and Whale Cloud built the data-sharing platform based on TM Forum’s best practices and standards, including the Business Process Framework (also called eTOM) and Information Framework (also called SID), the Open Digital Architecture and over 25 Open APIs.They also used guides relating to B2B2X partnering, partnership revenue models, metrics, blockchain use cases and data governance and monetization. Business scenario templates helped the team to map out business relationships and dependencies, and the AI & Data Analytics Maturity Model and Big Data Analytics Big Data Repository were essential for preparing the groundwork and defining data entities.China Unicom’s Zhijun Wang“TM Forum’s standards and best practices helped China Unicom avoid a variety of issues and reduce potential delivery risks. Thanks to the tools, we were able to achieve IT and business agility, simplified processes, competitive market response and excellent operational efficiency,” said Zhijun Wang, General Manager of Data Center, China Unicom.   China Unicom and Whale Cloud have been nominated for TM Forum's Excellence Awards 2020 in the Beyond Connectivity category. Click the logo to see a full list of award nominees. "
46,"TM Forum’s new report Network automation using machine learning and AI shows the importance of applying automation to processes across a communications service provider’s (CSP’s) entire organization, from the network to business units. Many of the processes will require artificial intelligence (AI) to achieve closed-loop automation, but whether AI-enabled or not, automation relies on data for intelligent decision-making to adjust parameters, alter configurations and understand users’ behavior.   The problem many CSPs face in leveraging data ties directly to their reliance on siloed architectures. Silos inherently keep data locked within. As it turns out, architectural silos breed organizational silos, and sometimes it is less a technical barrier to sharing data between groups as it is a political or territorial one. Group leaders are sometimes reluctant to share data with other groups because they fear it will be abused, especially customer data. This is untenable.Lack of standardsIn a survey conducted for our report, we asked CSPs around the globe why sharing data is so difficult. The top challenges are lack of defined and understood standard interfaces, identified by a full 81% of CSPs as a moderate or significant challenge, and not having a clear understanding about which data can and cannot be shared, identified by 77%.Data repositories may reside with third-party data warehousing providers, cloud-based analytics providers or internal analytics systems. Although TM Forum and others are working on open application program interfaces (APIs), there is not yet a standard for interfacing with repositories that may format data differently.Which data is OK to share?Not having a clear understanding of which data can be used or shared outside the organization with partners is a significant problem, which is also closely related to the No. 3 and No. 4 challenges of regulatory compliance and traceability. Not to be confused with explainability, which is understanding why AI systems come to the conclusions they do, traceability refers to CSPs’ concern about what becomes of data once it’s shared with authorized partners. Consumers can opt out of having their usage and behavior shared, but for those who do not, CSPs can share anonymized location, browsing and application usage information with marketing partners to develop targeted advertising.Operators need be able to verify that the data is not being shared with any non-authorized parties or further analyzed to try to associate the data with specific users. Partners must adhere to the rules under which the CSP operates, because the CSP is responsible.Addressing cultural issuesWhile uncertainty about internal data-sharing policies ranked relatively low on the list of challenges (about 60% said it’s a moderate or significant challenge), discussions with CSPs indicate this may be a bigger problem than it seems. Many operators said that internal policies around data sharing are either inadequate or non-existent within their organizations, and the also said that this is a cultural problem.Open APIs and a centralized framework for data distribution will go a long way toward solving the technical challenge and rendering the politics obsolete. In addition, CSPs’ senior leadership teams must push for creation of open data platforms that are accessible across their operating companies.This is the approach Axiata took beginning in 2016 to address a significant gap that had resulted from having separate data analytics teams for each of its operating companies. These teams had deep technical knowledge but lacked business understanding, and business leaders were not leveraging data to help with business decisions. The company established Axiata Analytics in 2017, appointed Pedro Uria-Recio Vice President and Head of the division and used the TM Forum Big Data Analytics Solutions Suite to assess data maturity and implement global best practices for big data analytics in all the operating companies.Read this case study to learn more about Axiata’s approach and watch for the new report, which will be published later this week."
47,"This is a snapshot of findings from our report How to leverage data across the entire organization derived from our tightly focused survey of 106 people from 46 unique CSPs representing every region of the globe, and who are responsible for executing data strategy. Download the full report now for further findings, our in-depth analysis as well as guidance on how CSPs can best leverage network and operations data.We asked CSPs which types of data are most valuable to their businesses, and it is not surprising that data about customers’ preferences and needs tops the list (see graphic below).Indeed, 66% of CSPs said it is extremely valuable to the business. This type of data can come from any customer touchpoint, such as call centers, retail or online stores, mobile apps, email, and so on. Network and operations data ranked very closely behind customer data, with 65% of CSPs saying it is extremely valuable to the business. We did not ask specifically how CSPs are using the data, but most use cases today are based around improving operational efficiency.Brand countsRemarkably, only about half of respondents said that data relating to brand and reputation is extremely valuable. If CSPs are going to make customer centricity the biggest driver of their transformation programs, they certainly need to measure the impact it is having on their brand. Most CSPs use Net Promoter Score to measure customer satisfaction, but data relating to brand can be even more useful.In our survey we put data from social media (what customers say about services on social media platforms and direct communication with CSPs via social media) in its own category, and it scored much lower than anticipated, with only about a third of CSPs saying it is extremely valuable data. This is perhaps because we did not explain that it could also include data about customers’ preferences and needs.The two other data sets we included as choices relate to CSPs’ supply chains and data about employees’ views and needs. When we asked respondents how successful they had been in collecting this data, there was a much higher success rate for supply-chain data than employee data (see graphic below).Hear more on the topic at our AI, Data & Analytics stream by joining thousands of your peers at TM Forum’s Digital Transformation World in our new Copenhagen venue this June."
48,"For decades, the year 2020 stood as a far-­oﬀ mile marker for technological progress that promised fascinating, even existential, changes. But here we are, and 2020 is like any other year: Technology companies continue to drive incrementally toward practical, proﬁtable improvements wherever they can be found. That doesn’t mean transformation is unnecessary, however. The communications industry may not be the creator of existential change, but it is one of the most important enablers of it as well as the social and productivity improvements that result from innovation. This report looks at what communications service providers (CSPs) and their suppliers are doing to keep advancing toward an unknown future with a more agile network that can support whatever comes.Here are some examples of changes we’ll discuss in depth throughout the report:Vendors increasingly are being asked to predict and model all potential outcomes for network-related services in order to automate the processes that enable them and create an autonomous, self­-healing, self-­optimizing network.As CSPs turn their focus to the enterprise market, they are looking to vendors to take on more of a partner role than simply a supplier.CSPs now have the leverage to insist on truly open architectures from their vendors, something suppliers have worked toward but with limitsAs the cloud model takes over, CSPs and vendors must come to terms with what cloud-­native really means and what should and shouldn’t migrate to the cloud.To adopt a platform business model, CSPs need help from their suppliers to extend assets to third parties in order to better monetize connectivity, billing, charging, hosting, etc.Access the related webinar for some lively insights and discussions around what can be done to change the working culture of telecoms operators."
49,"AIOps has gained traction as a concept because, according to the latest TM Forum AIOps survey findings, 60% of CSPs consider automation the primary objective for their operations as they push toward 5G and autonomous networks.  AIOps gained currency as a term when research firms like Gartner began to push it in the marketplace.Gartner describes AIOps from an enterprise IT perspective, writing it “combines big data and machine learning to automate IT operations processes, including event correlation, anomaly detection and causality determination.” But CSPs typically have a more complex variety of IT environments and software-driven networks to run, which expands AIOps’ scope for CSPs and forces a rethink of what IT for operations needs to look like in an autonomous-capable network powered with AI.CSPs face multiple sets of IT challenges: those faced by any large enterprise; CSP-specific business and operations systems and processes used to keep the lights on; and new software-defined networks. As C-level industry leaders call for change in the way CSPs do IT, each of these environments is being re-architected as cloud-native and moving to public, private and hybrid clouds. It is this major conceptual shift that has pulled AIOps into the spotlight.We must rethink ITOrange CEO and GSMA Chair Stephane Richard told the Digital Transformation World Series (DTWS) keynote audience that “IT transformation is an absolutely essential element of many operators’ strategies, including Orange” but also insisted that the telecom industry must “rethink the way we do IT.”Richard noted that IT has become recognized as a strategic capability. But board-level scrutiny reveals that “IT is too complex, too rigid, and too cost intensive,” he said. IT transformation projects are too slow and expensive as well.Richard called out billing, ordering, provisioning and other core IT functions for being monolithic. CSPs legacy debt is so heavy in fact, he said, that by 2025 “technical debts will consume more than 40% of operators’ current IT budgets.”The solution, he said, is for CSPs to make a “rapid shift to an open, modern, software-based technology architecture that enables new operating and business models.”This new architecture should be “loosely coupled, cloud native, and AI-driven,” he explained, and must be made of standard components that can be interchanged without customization. In short, he said, the industry “must evolve from a closed IT architecture to an open platform architecture.”ODA pivots operations to the cloud Transitioning operations to an open platform architecture is the purpose of the TM Forum Open Digital Architecture (ODA) and Open API manifesto. Richard points out that while aging standards processes may not be effective here, “complete code” and “real tests within the TM Forum” is what CSPs need to move to a fully automated, AI-driven IT infrastructure.In June 2020 the ODA took a remarkable step forward as a group of traditional OSS and BSS suppliers have joined more than 30 service providers including BT, Deutsche Telekom, Telefonica and Telenor, Chunghwa Telecom, Vidéotron and Globetom committed to the effort.The ODA working group states that it is “committed to transforming from legacy OSS/BSS to cloud-native software components and replacing traditional IT architectures with the Open Digital Architecture’s standardized plug-and-play components, data model and Open APIs.”Loosely translated, this means contributors to the ODA have been doing the practical work for several years to define how CSP operations can pivot to open, cloud-native architectures and standard APIs while helping CSPs to minimize both business disruption and stranded cost.AIOps is the next stepUnder the ODA umbrella, the TM Forum AIOps service management collaboration team is assessing how best to automate operations to achieve more speed, agility and cost efficiency as AI increases its role in operations automation. This work is necessary because “we have to move away from a traditional way of operating toward AI automation,” said Tayeb Ben Meriem, Coordinator of OSS Standardization for Orange, during TM Forum’s Digital Transformation World Series panel on AIOps. “That means we have to break silos that exist today, for instance from fulfillment and assurance, and we need to integrate all of this into a framework.”For a closer look at how CSPs are using AIOps to automate processes and AI to being the move toward autonomous networks, download the latest report from TM Forum: AIOps: From automation to autonomous networks."
50,"The value of data to communications service providers (CSPs) is not in question. What is unclear, however, is whether they are capable of leveraging it across the entire organization. Indeed, operators have been trying to come to grips with big data and data analytics for the last decade, and lack of a cohesive strategy is arguably the biggest obstacle to them becoming digital service providers (DSPs).  For this report we conducted a tightly focused survey of people working in data analytics roles within CSPs. Perhaps unsurprisingly, 88% of respondents said they consider the eﬀective use of data across the entire organization essential for their businesses. We explore how CSPs are collecting, storing and using data, and we oﬀer guidance to help operators leverage data to improve customer experience and optimize networks.Which CSP executives are leading data strategiesThe types of data operators collect and generate – and which are most valuableWhere CSPs store data and how it’s accessedWhat data lakes are and why they’re challenging to manageHow eﬀectively CSPs are using dataWhat the biggest barriers are to leveraging dataWhy data models are important but challengingWhy a strategy for data governance is necessaryHow CSPs can leverage network and operations dataHow CSPs can use data to improve customer experienceHow Axiata is aligning its data strategy across operating companies"
51,"The use of artificial intelligence (AI) and machine learning in customer value management has huge potential for communications service providers (CSPs).This offers the chance for more personalized communications and offers, resulting in lower churn and improved cross-selling and up-selling. But for CSPs to successfully embrace AI in customer value management (CVM) they need to make sure that their data, and approaches to storing data, are in a fit state and to prepare for the inevitable impacts on roles and responsibilities within the marketing function.This webinar explores in more detail the building blocks that CSPs need to put in place before they start to use AI, as well as the resulting benefits to CVM."
52,"The extraordinary technological advances taking place in sectors as diverse as robotics, sensors, data analytics, IoT, 5G, automation and artificial intelligence (AI) together constitute a new industrial revolution, one that is at least on a scale with its predecessors. But how and when will enterprises adopt these new technologies and capabilities? What examples do we have so far of how these different technologies – often used in combination – are being used? In short, how ready are enterprises for IR 4.0?This joint report from Fitch Solutions – a provider of country risk and industry research solutions and part of Fitch Group – and TM Forum, takes an enterprise-centric view of IR4.0. In addition to assessing three of the key technology components of IR4.0 – IoT, Blockchain and AI – it explores the supply chain for their delivery and adoption. It also includes a dedicated survey of attitudes to IR4.0 from leading manufacturers (TWI).Report written by: Mark Newman, Chief Analyst TM Forum, Nicholas Jotischky, Head of Custom Research Fitch Solutions and Paul Ridgewell, Independent Analyst.Research partner:"
53,"Communication service providers (CSPs) must deploy artificial intelligence (AI) throughout their organizations to thrive in the coming 5G era, and pulling that off will require a radical culture shift.That was the key message from Aaron Boasman-Patel, Vice President, AI, Customer Experience & Data, TM Forum, who kicked off the ‘AI and Data’ track at Digital Transformation Asia in Kuala Lumpur Tuesday by explaining why AI is a crucial component of 5G.In the short term, 5G is essentially a faster pipe, but that will change rapidly as the next phase of deployment enables massive internet of things (IoT) connections, ultra-low latency apps, and targeted vertical apps delivered via network slicing and edge computing.TM Forum’s Aaron Boasman-Patel speaks at Digital Transformation Asia“The complexity is going to increase in our networks, which means we have to now start deploying and using AI to get us over that complexity,” Boasman-Patel explained. “Everything has to now become more zero touch. We need to have open architectures that are based using microservices, where we can use things such as APIs to transfer that information across.”To date, CSPs have deployed AI technologies in some form or other in the network management space – for example, cellular site planning, reducing truck rolls, and customer onboarding. However, the ‘AI telco’ of the near future will have to deploy AI technology across the entire business, and manage that AI at scale in real time. That includes having the ability to explain to customers why the AI made a particular decision, especially if that decision leads to a network problem of a bad customer experience.It also includes proper data governance – ensuring datasets are clean, algorithms are properly trained for those datasets, and managing all of that across the lifecycle. That also means opening up all data in different business silos so that it can be easily accessed in the first place – something that many CSPs currently don’t do.Naturally, getting from here to there is a major challenge. Boasman-Patel highlighted a number of programs TM Forum is currently offering to help CSPs address these challenges, such as a standardized data model that provides a common platform to ingest data in the same format, as well as a training repository, AI management standards to help manage AI at scale, and a maturity model to determine how ready a CSP is to start adopting AI effectively.There’s also the AI for IT & Network Operations (AIOps) Catalyst which enables telcos and vendors to explore AI-based use cases for optimizing CSPs’ processes and services to improve customer experience, quality and efficiency.However, the prerequisite step for adopting AI effectively is for CSPs to change their approach to the business. Acquiring the right skillsets is an obvious necessity, but that doesn’t mean much without establishing the right cultural mindset to make the best use of those skill sets, Boasman-Patel said.“We’ve got to have a cultural shift and a radical rethink of the way we’re doing this,” he insisted. “We’ve got to change to a culture which is much more open; we’ve got to have a culture of experimentation.”"
54,"For many people, artificial intelligence (AI) still seems like part of the future, not the present, but its use is already becoming a reality in telco networks. There is talk that AI will be used to run network scaling capabilities (in&out or up&down) and to operate zero touch service operations centers. But is this realistic?Well, according to research from 2018 by the TM Forum Digital Transformation Tracker 3, one-third of communications service providers (CSPs) have started to deploy virtualized network functions (NFV) at various parts of their networks. We will only see this trend grow. A number of operators have announced plans to become fully software-defined in the future including T-HT, a part of the Deutsche Telekom group in Croatia, recently announced on LinkedIn that it would be in that position by the end of 2022.Predicting the futureIf AI is part of the future—or even the present—where will CSPs look for a partner to provide network analytics? TM Forum polled a group of CSPs and found that:54% said that they would manage with existing resources;40% would find a new technology partner with a traditional telco-oriented portfolio; and38% were planning to find a new technology partner specializing in AI.I think there are three possible outcomes of this. The first, that network equipment providers, traditional telco-oriented software and appliance vendors will incorporate AI and machine learning (ML) technologies (mainly open source) into their products. This will obviously have to be at the level of  today’s leading AI and ML specialists, otherwise they will fall behind.The second option is that the AI and ML specialists will figure out how to communicate with, and appeal to telcos— understanding the complexity of telecoms and learning to speak the language. They can then help CSPs take advantage of their currently-superior technology to support networking.There is, however, a third, riskier option. CSPs bring in a combination of open source technology and new data scientists fresh out of university into network operations and planning teams. Drawing on big data, this innovation-led combination could actually start to rule the world. After all, great risk comes great reward — in this case, a competitive advantage.The price of failureHowever, the price of failure could be falling behind competitors that went in the ‘commercial off the shelf’ (COTS) direction. Every COTS company playing in the AI and ML domain is currently throwing billions of dollars into research and development. Why pay for your own development and worry about changing versions of different integration points, when specialists could do it for you?This issue is probably why Gartner is predicting that by 2022, 75% of new end-user solutions leveraging AI and ML techniques will be built with commercial solutions, rather than open source platforms. However, open source functionalities will be part of the AI and ML learning solutions to run on those platforms. Business get value from data by being able to feed it to virtualized network function managers, who can then to scale the throughput up or down as necessary. The only systems that can do this are those that can forecast in real-time, detect anomalies using advanced neural networks, and use deep learning and other advanced machine learning algorithms via standardized APIs.Right now, some Tier 1 and a few Tier 2 CSPs are starting to think about these options, experimenting with partnerships with research departments at universities, using their big data resources. Leading AI and ML software vendors have the necessary capabilities, but may not have the specific telco and network data knowledge. Both seem to have the wheels and engine on one hand, and the steering wheel on the other. The best option would likely be to bring the two sides together, rather than each trying to build the missing parts from scratch.Getting the best value from AIAccording to Facebook speech on TIP summit from 2018, capex savings of up to 10% can be achieved just by using AI to drive more efficient network planning. Much more can be achieved if – instead of provisioning networks to function for peak traffic, providers use scaling capabilities running on commercial software in the cloud. Knowing when to switch on and off in advance, and even detecting anomalies before customers notice the impact, will make all the difference between good and bad customer experiences.I am often asked which statistical method or ML algorithm is best for predicting a particular type of network problem but it’s almost impossible to answer this. The many challenges and anomalies in any network that are below the alarm threshold make it almost impossible to determine which algorithm to use without actually testing some of them out. By finding the one that delivers the best results, we can run automatic optimizations of predictive models and results. Many analysts agree that the best option is likely to be some sort of enterprise AI and ML platform, coupled with the flexibility of open source technologies running on top of this platform. This has both the power and the flexibility to handle complex problems, like detecting and predicting a problem with a VoLTE call and establishing its root cause. This may sound simple enough, but even incompatibility between different voice codecs in different VoLTE networks can lead to a problem, and that’s before you have even considered a VoLTE call to a non-VoLTE device.How, therefore, should telcos make decisions about where and how to invest in AI?First, it depends on your company’s business goals, because AI and ML can provide results that are measurable in dollars. But, it also depends on the AI and ML competences available to you, either in-house or via strategic partners. The best option may be to trust the technology and your platform of choice, and create an agile team involving both your people and those from your technology provider to move fast and solve each challenge as they arise. Throwing all your data at the vendor might get better results in the short-term, but it may also mean giving up control of the business’s best asset to somebody else. To avoid this will mean investing in new skills and capabilities internally."
55,"This report looks at the need for CSPs to better extract and incorporate outside plant data, including geographic information systems (GIS), into the bigger operational picture. This need is especially acute with 5G transformation underway, aided by new tools like machine learning and artificial intelligence (AI), which are often grouped together under the term AI.While the title of this report might imply that using AI is mostly about capital expenditure (CapEx) and cost savings, there is much more to outside plant intelligence than reducing costs. Maximizing CapEx investment is about CSPs getting the biggest bang for their buck.Read it to understand:Where CSPs are applying AIWhy AI is important to outside plant engineersHow outside plant data helps other departmentsWhy application program interfaces are an important first step to an open AI environmentWhy the fronthaul portion of the transport network demonstrates the need for data integrity in outside plant"
56,"Artificial intelligence (AI) proponents and skeptics alike should be comforted by the activity taking place within the TM Forum Collaboration Program aimed at developing policy, processes and rules for AI management and assurance.Taking a refreshingly sober approach, the AI & Data Analytics team met this week in Dallas at TM Forum Action Week. The group is working to advance the use of AI in telecom by protecting users, operators and customers at the outset. No other emerging technology has prompted developers to put so much effort into limiting, throttling, monitoring and protecting against ill effects, before unleashing it on the marketplace.Yet, if done properly, the protections the team is currently building into AI management specifications for testing, training and certifying its AI models will ultimately enable better adoption because resulting solutions will have been built above all for trust.Ensuring AI explainabilityRob Claxton, Chief Researcher at BT Research, explained that to create AI management standards and manage automation in AI, the industry needs to build a framework to ensure the technology is used correctly, safely and reliably – and that it is explainable and accountable.BT Research’s Rob Claxton“The problems we want AI to solve are very critical from a business point of view,” Claxton said. “If we are going to do this at scale, we must have the methods and frameworks for managing it,” Claxton said.The AI collaboration team is trying to protect against building models that are too specific and support only a narrow set of applications, while also allowing them to be adaptable for an unknown future. But the models also must not be too general.The group is working on processes for tracking AI throughout its lifecycle to gauge changes in live models versus the configuration when originally deployed. When deploying AI, operators need to understand the constraints placed on the technology and be able to test at any point in the model’s lifecycle whether those constraints are true and still hold.TM Forum is driving a range of best practices, thought-leading collaboration activities and research, in addition to services such as training and coaching in data analytics and realizing the business value of AI. The project teams are currently working on an AI data model for telcos, management standards for AI, creation of an AI data training repository, an AI maturity model and metrics, and AI user stories. To learn more or to join the group, please contact Aaron Boasman-Patel."
57,"Sunny Nirala, Big Data Programme Manager, Celcom Axiata BerhadSunny Nirala, Big Data Programme Manager, Celcom Axiata Berhad, and finalist for TM Forum’s ‘Future Digital Leader’ Excellence Award 2019, explains his role in orchestrating and implementing a robust platform to support all of Celcom’s reporting and analytics requirements How would you describe your organization’s drive towards enterprise-wide digital transformation?Celcom’s drive embodies being able to deliver excellence in experience for all participants in its value chain. This includes being able to bring disparate parts of the organization and its eco-system together, in a digitally-led world. In order to achieve this, big data and data analytics sits at the heart of the way the business is driven – being able to understand what has happened in the past and the future effects of it, as well as the actions that are required. What has your role been in helping to deliver on these outcomes?As Celcom’s Big Data and Analytics Programme Manager, my role was to build a new platform from the ground up; something which would be robust enough to support all of Celcom’s current and future reporting and analytics requirements, as well as something that was capable of IoT, machine learning and deep learning in the IR4.0 era. To enable this, a profound change was required in the way people, processes and technology interrelatedly operated. What new technologies or IT systems have you put in place which have impacted a change in working and operational practices?An enterprise-wide data lake now means that anyone, in any area of the business, at any time, can access the same data and the same business rules, subject to the various data governance and security controls. This affects the entire way of working, for example, analytics and data science teams are now able to develop predictive models based on a wider set of data, thus leading to increased accuracy and efficiency. In addition to this, other parts of the business use the data lake for descriptive analytics through self-service analytics. The magic here is that everyone gets to access the same data sets. How does TM Forum help you to achieve your digital transformation objectives?TM Forum provides a platform where all telecom operators across the globe can collaborate and bring a versatile set of parameters to the table, to push the boundaries of current thinking. This way of thinking helps to evolve the way we carry out our day to day lives, and ultimately helps to achieve paradigm shifts in the way we operate. In order to facilitate these shifts, TM Forum’s Information Framework (SID) and the various maturity assessments have been invaluable. Max Planck, the theoretical physicist said it well: “When you change the way you look at things, the things you look at change.” Digital transformation is not just about changing processes and technology; it also involves a shift in organizational culture. How is your teamwork changing and what’s your role as a team leader?It is true that processes and technology do not bring about the shift required in businesses today, however it is the thinking on which they base themselves. The culture which ensues beyond the implementation of various processes and technologies is of paramount importance. This means bringing together different parts of an organisation to achieve a single vision and importantly, ensuring the changes last.Through the implementation of the data lake, teams across the organisation that had previously operated in silos were required to bring their individual sets of problems to the table to define a way forward. As the team lead, I saw my role as pivotal in bringing everything together. It was important that I understood the professional, operational, environmental and cultural dynamics to build a future that would impact the principle modus operandi as well as the results that would be seen from it. Leadership is clearly paramount, but what makes a good digital leader?A good digital leader is one that knows the way, goes the way, and shows the way. Digital leadership is about being passionate about the things that matter, and setting a clear vision with integrity, honesty and humility. It is about developing the skillsets required across the organisation to operate in an ever-evolving digital world. Digital leaders operate in an environment where everything is subject to change, where new technological advancements are happening exponentially and where tomorrow’s world dawns upon us today. The digital leader must be able to navigate through an ever more connected world. In terms of the wider societal impact of digitization, what are you looking forward to over the next decade?Over the next decade I look forward to the continual evolution of new technologies, and technologies that do not exist today, which will enable the physical world and digital world to inch closer and closer together. The way in which we carry out our day to day lives will continue to be transformed, as will the structure of how society lives and operates. The modus operandi will continue to be challenged and paradigms will evolve faster. The impact to society will require people to look beyond physical boundaries, and instead focus on evolving levels of thinking that bring the physical and digital worlds closer together. In terms of the wider societal impact of digitization, what concerns you?There are many factors that digitization will have, to the continued transformation of society at large, including the further penetration into our current ways of thinking and operating. Society will continue to evolve and will take along with it those that are able to adapt to the social, legal, economic, political and technological changes that are likely to take place. Ultimately at the heart of all of the change, will be the requirement to ensure that a continued learning approach is embodied. To meet more Excellence Awards 2019 Outstanding Achievement finalists and read about their transformation journeys, go to https://www.tmforum.org/about-tm-forum/awards-and-recognition/excellence-awards/finalists/#tab3"
58,"A logical starting point for transformation and automation is with fundamental support processes, and few are more fundamental than the inventory systems that store data about physical and virtual network resources. Inventory is part of the essential structure of a network that enables all the services layered on top. Dynamic inventory systems track and manage resources in real time using federated data. This approach is critical for intelligent automation.This report looks at the role dynamic inventory plays in making every other process and system more accurate and therefore easier to automate. Read it to understand:How inventory systems have evolvedWhat dynamic inventory is and how it can be used to enable intelligent automationHow CSPs are federating data across multiple OSS and domains to drive automation and improve customer experienceThe role for artificial intelligence and machine learning in dynamic inventory and automationSteps to take to move toward intelligent automation"
59,"Although challenging, there is an opportunity to make a measurable impact to a CSP’s bottom line by identifying and addressing problems during the order-to-activation process.This report assesses the benefits that can be achieved by using real-time operational analytics to improve ordering and delivery of new services or when reactivating services. We consider the pressing requirements for service providers as they go to market with new internet of things and 5G services, which involve on-demand activation, in many cases at scale, as well as a bewildering new array of devices and partners.Read this report to learn:Why order fulfillment and activation, and operational intelligence from them, are so importantWhy visibility must be extended down to individual customers’ journeysHow big data analytics and platforms interpret the typically complex stories told by the dataThe opportunities and efficiencies promised by 5GHow Vodafone UK and CenturyLink in the US have benefited from real-time operational analytics"
60,"Much of the enthusiasm around artificial intelligence (AI) has been replaced by caution and indecision as communications service providers (CSPs) grapple with the many difficulties of applying AI to operations (AIOps). Still, they are beginning to make progress.As networks become more complex, the IT systems supporting them also must evolve to accommodate future demands. Networks and IT must be prepared to accommodate billions of connected devices and waves of new technology. CSPs recognize that AIOps will be the only way to scale operations and make them sufficiently fast and responsive to profoundly changed networks and businesses as digital transformation progresses.In researching this report, we surveyed 65 executives from 37 different global, regional or national CSP operating companies in 25 countries, and 48 suppliers from 33 unique companies. We also conducted in-depth interviews with operators and suppliers. Read this report to understand:Which operational changes CSPs are undertaking that require AIThe types of AI technology operators are using in operations and for what purposesHow reliant operational transformation is on AIThe use cases driving AIOps deployment, including which of them operators are implementing today versus in the futureIssues and gaps CSPs are finding in their attempts to harness AI"
61,"To improve customer experience (CX) and increase average revenue per subscriber (ARPU), communications service providers (CSPs) traditionally have relied on rather rudimentary tools like surveying customers to determine whether they’re satisfied. More recently, operators have begun using customer relationship management (CRM) platforms such as Salesforce to collect data about customers’ interactions. Now a TM Forum Catalyst proof of concept called MindReader is taking this a step further by tracking customers’ intent using application program interfaces (APIs). The idea is to create a more accurate picture of customer experience that can be used to personalize services and create new ones.Telstra is championing the Catalyst, which is being demonstrated this week at Digital Transformation Asia in Kuala Lumpur. Participants include:CloudSense, supplying intelligent commerce, real-time decisioning and CRM on SalesforceInfosys, providing orchestration, intent intelligence, intent qualification management and APIsNokia Australia, delivering the service management platform, digital intelligence and intelligent customer agent on SalesforceLearn more about TM Forum Catalysts Rapid-fire, proof-of-concept projects connecting service providers, technology suppliers and global enterprises to create innovative solutions to common industry challengesHere’s how it works:A customer interacts with a CSP (for example, by exploring and purchasing products, using them, experiencing troubles, churning, etc.) through the Cloudsense Intelligent Configure Price Quote (CPQ) system. In this context, the interactions are called ‘episodes’.The customer is prompted at key points during the interaction for guidance through questions or selections of options through the Cloudsense CPQ graphical user interface. An ‘intent’ is captured at each salient point during the interactions through explicit questions and answers or usage patterns and recorded via the Infosys Intent qualification management, including the episode information and association in the intent management data store (in this case a data lake because of the volume and velocity of data).The intent is ranked based on parameters such as relevance, frequency and value. The function is performed centrally in the data lake and also by the individual AI components while using the information in the context of episode – for example, Nokia Service Management Platform (SMP) and omnichannel capabilities for assurance episodes, Cloudsense for sales episode, intent qualification management for usage and product evolution episode.The ‘intent object’, which is created as a result of the first two steps, is used to refine subsequent interactions with the customer for the episode (refinement location depends on the episode type and customer interaction – in this use case context in Cloudsense for next-best offer selection and Nokia SMP for assurance episodes) as well as others, and the most relevant intent is selected to drive next-best actions for offering the customer a personalized experience.Intent insight is offered as a service internally from the Infosys intent qualification central data lake (and exposed externally as a product/service via the Cloudsense CPQ) so that any system from the network to customer-facing apps may call the service to deliver intelligence and relevance in real time for customer interactions.Intent as a service is also available for product managers across the enterprise to help them evolve products. Eventually, it could be sold to other businesses as an offering configured in the Cloudsense CPQ.As part of the project, the team has created two new APIs for intent and episode management, which they plan to contribute to TM Forum as Open APIs. In addition the project makes use of other Open APIs for service catalog and ordering and for activation and configuration. The team also plans to  contribute the intent and episode objects to the TM Forum Information Framework.Making sure customers get what they wantCSPs have always captured intent and episode data, but they haven’t been able to track or utilize it efficiently because it often gets lost in interaction notes or call detail records. By making the data more accessible and useful, customers will benefit from better, proactive customer experience and new services suited to them specifically.“We can use this intent information to ensure that we are providing a customer what they actually wanted, which may differ from what they have asked for,” says Andrew Davison, General Manager Enterprise Product Software Development, Telstra. In addition, by logging and tracking intent, the operator can retrospectively qualify customers for new services as they become available. This knowledge of what customers have asked for in the past can be used to build sales leads.“We recognize that as we work towards a technology platform that is a federated set of small, loosely coupled services which we assemble into product offerings, it quickly becomes difficult to trace specific service actions, or intent episodes, initiated by users because there is no way to trace an API call with certainty back to a particular interaction with the customer (whether driven by an online portal, an app or service staff),” Davison adds. “So, we need to be able to propagate a label for the interaction down through the technology federation to make it simple and fast to recall, and profile all network and system interactions that occur as a direct consequence of the customer interaction.”At Digital Transformation Asia, the team is hoping to get feedback from other CSPs on the new intent and episode management APIs and on the idea for delivering intent as a service. They plan to continue their project at Digital Transformation World in Nice, where they will test the APIs and introduce big data, artificial intelligence and business intelligence into the solution.“We are planning to be involved in future phases of the work, to build out an end-to-end service prototype which allows us to demonstrate the power of intent propagation in better understanding what is happening in a complex, federated delivery environment without needing to engage data-sifting or probabilistic algorithms,” Davison says.If you’re interested in participating in the project, please contact Tania Fernandes via [email protected]."
62,"There is a strong argument that the telecoms industry needs to come together in the same way to embrace common approaches to collecting, organizing and distributing the zillions of bits of data that are being generated all the time, and that could be driving better decision-making, automation and customer experience, if properly harnessed.Operators are struggling with their own internal processes to figure out how to apply analytics, machine learning and, ultimately, more sophisticated artificial intelligence to this massive amount of data, even as the coming boom in connected devices will exponentially increase the amount of data generated every second of very day.Now is the time to consider a more standard approach to the architecture for managing data, in the form of an industrywide consensus on how data is collected and managed to more rapidly enable a digital transformation, powered by data analytics. And it makes sense to start with standardizing the approach to data lakes and data warehouses, as they become critical elements in communications service providers’ (CSPs) strategies.In this Quick Insights report, we’ll explore why common data models and a standardized approach to setting up data lakes is of benefit to CSPs and the telecoms industry in general. The good news is that many of the pieces to enable this process are already in place.Read this report to:Find common data models that can prevent the failure of digital transformation programs when combined with a staged approach to standardizing how data is classified, collected, distributed, managed and used to drive decision-makingHear the experiences of operators including Verizon Communications and Telia CarrierUnderstand why an industry-agreed approach to making sense of ‘event data’– the information constantly generated within networks and operations – is so important and the benefits it would deliverUnderstand the importance of open application program interfaces (APIs) in taking a scalable, standardized approach to virtualizing dataExplore what standards are available now and how they can be applied to virtualizing data"
63,"Data mass is beginning to exhibit gravitational properties – it’s getting heavy – and eventually it will be too big to move. How will networks handle this? And what does it mean for digital identity, privacy and trust?Those are questions Guy Lupo, General Manager, Head of NaaS 2020, Telstra, has been asking as he ponders the future of digital transformation. He has some ideas about the answers.Lupo has created an interesting slide that envisions an intelligent distributed network powered by open APIs (see below). He discussed it at TM Forum Action Week recently during a session focusing on the tension between the need for ubiquitous availability of data and the expectation of trust that customers (and in many countries, regulators) have about how data is used. This juxtaposition is something communications service providers (CSPs) and their partners must consider as they develop network and IT architectural models, including the Open Digital Architecture.Centers of data, not data centersTelstra’s Guy LupoLupo suggests that as data becomes too big to move, it will become centralized.“If you look at IoT, where hundreds and thousands of sensors are creating massive amounts of data, it’s getting too big to move – we are starting to have centers of data, not data centers,” he says. “And if you have centers of data…you will be waking up to a world of APIs.”Lupo predicts that in the future everything will be ‘as a service’, automated and accessible via APIs. In addition, everything will have a digital identity with a trusted computing base so that data can be pulled from centralized locations.“In the next six to ten years, there will be identity for sensors, identity for data, even identity for parts of your data records,” Lupo says. “You will be able to expose parts of your health record because your data elements will have an identity.”Once data is centralized, he expects that “AI agents” will travel between the centers of data, learning and executing policy, without carrying copies of the data with them, only the learnings: “Data and learning are localized, and AI moves around making decisions and taking decisions within the geographical jurisdiction of the governing body that owns the data.”The time is nowIt’s critical to start thinking about data security and privacy before it’s too late. Once AI agents are traversing networks and making decisions, it will be too late to overlay architectural principles. Lupo says we need to consider data in terms of minimal exposure, privacy, confidentiality, longevity, availability, integrity and digital footprint.For example, most people have no idea how large their digital footprint is.“Do you really know how much of you is out there?” Lupo asks. “Google is your public digital footprint… At the moment, the cost of compute is not low enough for Google to be able to afford a CPU dedicated to you, but at some point they will have one. At that point you won’t be able to hide in plain sight inside statistics as you do today.”Realms matter when it comes to data privacyLupo and other TM Forum members discussed the role the Forum can play in helping CSPs navigate these issues of rapidly increasing amounts of big data, network and IT transformation, and customer privacy and trust.“There is no language around this, so there’s an opportunity within the Forum to talk about it because we sit in the IT realm and are the hosts of all this data,” Lupo says.One important area of focus could be on how data can be used within specific realms. In many architectural models, including ODA, there is supposition that data will be ubiquitously available as part of a giant data lake, but there is no real understanding or agreement about how trust requirements may affect the use of this data.In the EU, for example, strict rules about customers’ privacy are enforced through the General Data Protection Regulations (GDPR). These rules give consumers a right to privacy, a right to be forgotten, and often specify in which realms data can be used.“If you’re using data for something you’ve got permission to use it for, you can put as much data into the data lake as you like,” says George Glass, VP, Architecture & APIs, TM Forum, who before joining the Forum was Chief Systems Architect at BT. “But you can run into problems even when you think you’ve anonymized the data.”Anonymizing data isn’t enoughGlass gives a real-world example of a GPS satellite navigation system, which uses anonymized subscriber data to improve route guidance and give users driving directions based on time of day, typical patterns of congestion, etc.He explains: “The data is completely anonymized. However, if [as a CSP] I’ve got the GPS data and I can link that to mobile phone data, which also gives me location, I can very, very quickly work out what the anonymization ID is of an individual, and I can tell you exactly where he’s been based on his GPS information. If I bring the anonymized, safe GPS data into my data lake and start processing it and link it to the individual, he is going to get very agitated that I know where he’s been because I don’t have his consent to use the data in that way.”Forum members who are working on ODA could explore this idea of realms by dissecting an example like GPS, exploring how data is stored and processed using machine learning, and looking at decisions that are made and implemented using AI.“We’re starting to talk about data, machine learning and AI, and we’re also talking about business practices in terms of owners of data and purpose of use within a realm,” Glass says. “We should be coming up with patterns or guidance on what you can do with the data – or to say these are the kinds of things you can do with the data if you join two realms.”If you’re interested in participating in this work, please contact George directly via [email protected]. To learn more about Guy Lupo’s views on the future of digital transformation, check out his blog."
64,"When customer experience or operations teams within communications service providers (CSPs) want to adopt artificial intelligence (AI), they first must find a way to ‘sell’ it to finance executives, and perhaps more importantly to the employees who could be displaced by the technology. Neither is an easy job, according to a panel of experts gathered here at Action Week in Dallas.Jerrid Hamann, Digital Customer Experience Strategist, Verizon, who has worked for the company for about a year, spoke about his experience at another telco where he was trying to implement AI for customer service. It was a struggle.“I was trying to convince finance that we needed the new tools to improve customer experience,” he said. “Either they were very skeptical and suspicious saying it sounds like science fiction and is not something we want to invest our money in…or at the other end of the spectrum they say, ‘Oh wow, we can save that much money? Let’s lay off the entire contact center’. When you get that kind of reaction you have to dial it back and explain that it’s something that has to be phased in.”Calming fearsThe sell was even more difficult with the customer service teams that could be displaced by AI chatbots.“When we tried to introduce the idea, we certainly got a lot of pushback from the folks in the contact center who said, ‘This looks like a threat to our livelihood’,” Hamann said. “We tried to find ways to soften it, telling them that we were looking to offload the most mundane kinds of transactions so that they could handle more complicated customer issues.“We even talked about using AI as a coach for contact center agents – so letting them know we weren’t getting rid of anyone but that the platform might help them resolve an issue much more quickly,” he added. “We tried a few different ways to position it as ‘robots are not coming for your job’, but certainly that is the initial reaction… ‘I need to feed my family, so please stop what you’re doing’.”Climbing the hype curveThe other panelists, who work for suppliers, noted that AI can, indeed, be a tough sell in some parts of the CSP business and stressed that AI is making its way through the infamous ‘hype cycle’, a term coined by Gartner. Indeed, the 2018 Hype Cycle for Emerging Technologies shows that AI hype hasn’t even peaked yet.Augmented intelligenceWhen evangelizing AI internally, CSPs need to stress that the technology is not just a replacement for humans, the panelists noted. It can make humans’ jobs more interesting and worthwhile, and humans are still needed to understand data.“At IBM we don’t like to say ‘artificial’ intelligence; we say ‘augmented’ intelligence,” said Janki Vora, Thought Leader in AI, IBM. “It’s not about taking away jobs; it’s making it better and faster. But at same time, IBM and some of our telco customers are investing in reskilling and retooling so that [displaced workers] can do something more complex… It’s a new era of different kinds of jobs that can be better and safer.”Avai S, Director of the IoT, Big Data, AI/ML, Microservices Practice at Prodapt, agreed. “Employees should be made comfortable,” she said. “Without them we won’t be able to create value from the data.”Russ Bartels, Director, Analytics, SDN & Network Automation,Windstream Communications, acknowledged that some attrition will result from implementing AI in customer service and operations, but like Avai he stressed that it’s crucial for CSPs to employ humans who understand the data.“If someone’s great at what they’re currently doing and they’re willing to learn, odds are they are going to be able to continue working in another area,” he added.Blueprint for successVerizon’s Hamann said most CSPs don’t have a blueprint for success in implementing AI yet.“We didn’t get far enough in our journey to prove out [the benefits],” he said. “We had some fits and starts and stops along the way, so we didn’t really get a chance to prove what we’re saying is true. In our case the skeptics kind of won…so it would be nice to have another crack at it because the technology has improved; the software has improved.”Panel moderator Aaron Boasman-Patel, VP, AI & Customer Centricity, TM Forum, suggested that some of the onus is on suppliers to help operators develop a successful strategy for AI deployment. “What responsibility do you have to get it right?” he asked the panel. “Service providers will just get fatigue if they have bad experiences.”Avai said it’s important for vendors to help CSPs establish not only short-term use cases for AI but also long-term business cases. “They are not the same thing,” she said.Vora noted that success in other companies can help persuade CSPs as well.“We may have very few CSP success stories that we are sharing, but there are companies out there that have AI in their DNA – for example, Alexa and Google Home. Those companies have embedded [AI] in their culture.”Establishing trustGiven AI’s long road ahead, how can CSPs and suppliers establish the trust needed to make it a success story? Boasman-Patel asked.“You’ve got to prove that it works,” Windstream’s Bartels said. “Your leaders need to believe in it and drive it, and you have to realize you’re not going to be successful every time.”Verizon’s Hamann suggested that building AI trust is like elementary school science.“You have a hypothesis and try to prove or disprove it,” he explained. “Whether you use AI to prevent network failures, help customers or create marketing offers, you have a control group where you’re not using AI and a group where you are using AI and see which one is better.”He adds: “You need to try to be as dispassionate as possible. That way it’s a transparent process. You’re not cheerleading AI. You’re not trying to hype it or promote it. You’re just experimenting.”Vora believes defining what trust means is key.“A lot of organizations don’t have a definition that’s clear for everyone about what trust means,” she said. “Sometimes data is too guarded, but on the other end of the spectrum you need to have governance set up to make trust possible. The bottom line is that in order to trust AI systems, we first need to come to define what that trust definition is.”Finally, Prodapt’s Avai pointed out that trust is two-sided.“You have to have confidence in AI systems – that’s more of a technical trust,” she said. “The second is people trust – senior management and employees have to trust in AI.”"
